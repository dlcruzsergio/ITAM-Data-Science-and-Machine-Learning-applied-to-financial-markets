{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='margin-bottom:20px;'>Data Science Module</h1>\n",
    "<p style='margin:0;padding:0;'>Sergio de la Cruz Badillo</p>\n",
    "<p style='margin:0;padding:0;'>dlcruzser12@gmail.com</p>\n",
    "<p style='margin:0;padding:0;'>06/Enero/2020</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification (Text Categorization or Document Classification)\n",
    "\n",
    "The goal in classification is to take an input vector $x$ and to assign it to one of $K$ discrete class $C_k$ where $k=1, ..., K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge pdfminer.six -y\n",
    "#!conda install -c conda-forge pattern -y\n",
    "#!conda install -c conda-forge gensim -y\n",
    "\n",
    "import importlib\n",
    "if importlib.util.find_spec('pdfminer') is None:\n",
    "    print(\"*******************************************************************\")\n",
    "    print(\"**** ¡WARNING! ***** ¡WARNING! ***** ¡WARNING! ***** ¡WARNING! ****\")\n",
    "    print(\"*******************************************************************\", end=\"\\n\\n\")\n",
    "    print(\"Instalación de 'pdfminer' con conda para python 3 'conda install -c conda-forge pdfminer.six -y'\")\n",
    "if importlib.util.find_spec('pattern') is None:\n",
    "    print(\"*******************************************************************\")\n",
    "    print(\"**** ¡WARNING! ***** ¡WARNING! ***** ¡WARNING! ***** ¡WARNING! ****\")\n",
    "    print(\"*******************************************************************\", end=\"\\n\\n\")\n",
    "    print(\"Instalación de 'pattern' con conda para python 3 'conda install -c conda-forge pattern -y'\")\n",
    "if importlib.util.find_spec('gensim') is None:\n",
    "    print(\"*******************************************************************\")\n",
    "    print(\"**** ¡WARNING! ***** ¡WARNING! ***** ¡WARNING! ***** ¡WARNING! ****\")\n",
    "    print(\"*******************************************************************\", end=\"\\n\\n\")\n",
    "    print(\"Instalación de 'gensim' con conda para python 3 'conda install -c conda-forge gensim -y'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import re\n",
    "import requests\n",
    "import scipy.sparse as sp\n",
    "import string\n",
    "import sys\n",
    "import urllib.parse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from io import open\n",
    "from multiprocessing import Pool\n",
    "from numpy.linalg import norm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pattern.es import tag\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sergio.delacruz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Publication:\n",
    "    \"\"\"\n",
    "    A class used to get information about the Banxico publication\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    date_view: publication date\n",
    "    text_view: publication text view\n",
    "    label: publication status or label ('mantiene', 'disminuye' and 'incrementa')\n",
    "    url: pdf publication file url\n",
    "    text: pdf publication text\n",
    "    normalized_text: pdf publication normalized text\n",
    "    token_list: token list from normalized text\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(date_view, text_view, label, url, text, normalized_text, token_list): constructor method\n",
    "    __str__(): to string method\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, date_view, text_view, label, url, text, normalized_text, token_list):\n",
    "        '''\n",
    "        Constructor method\n",
    "        '''\n",
    "        self.date_view = date_view\n",
    "        self.text_view = text_view\n",
    "        self.label = label\n",
    "        self.url = url\n",
    "        self.text = text\n",
    "        self.normalized_text = normalized_text\n",
    "        self.token_list = token_list\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''\n",
    "        To string method\n",
    "        '''\n",
    "        print('date_view: {0}'.format(self.date_view))\n",
    "        print('text_view: {0}'.format(self.text_view))\n",
    "        print('label: {0}'.format(self.label))\n",
    "        print('url: {0}'.format(self.url))\n",
    "        print('text: {0}...'.format(self.text[:24]))\n",
    "        print('normalized_text: {0}'.format(self.normalized_text))\n",
    "        print('token_list: {0}'.format(self.token_list))\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "Web Scraping (also termed Screen Scraping, Web Data Extraction, Web Harvesting etc.) is a technique employed to extract large amounts of data from websites whereby the data is extracted and saved to a local file in your computer or to a database in table (spreadsheet) format.\n",
    "\n",
    "Banxico URLs:\n",
    "- https://www.banxico.org.mx/\n",
    "- https://www.banxico.org.mx/publicaciones-y-prensa/anuncios-de-las-decisiones-de-politica-monetaria/anuncios-politica-monetaria-t.html\n",
    "- https://www.banxico.org.mx/publicaciones-y-prensa/anuncios-de-las-decisiones-de-politica-monetaria/{file_name}.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    '''\n",
    "    Crawler class contains the main processes to get information from a website.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    session: publication date\n",
    "    url_base: publication text view\n",
    "    headers: publication status or label ('mantiene', 'disminuye' and 'incrementa')\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(): constructor method\n",
    "    __str__(): to string method\n",
    "    get_page(url): to get BeautifulSoup with information about a website\n",
    "    get_publication_list(bs, processes_num=0): publication list from banxico\n",
    "    read_PDF(file_url): to read text from a pdf file\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructor method\n",
    "        '''\n",
    "        self.session = requests.Session()\n",
    "        self.url_base = ('https://www.banxico.org.mx')\n",
    "        self.headers = {'User-Agent': ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) '\n",
    "                                       'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                                       'Chrome/39.0.2171.95 Safari/537.36'),\n",
    "                        'Accept': ('text/html,application/xhtml+xml,application/xml;'\n",
    "                                   'q=0.9,image/webp,*/*;'\n",
    "                                   'q=0.8')}\n",
    "    def get_page(self, url):\n",
    "        '''\n",
    "        To get BeautifulSoup with information about a website.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        url: website url\n",
    "        '''\n",
    "        try:\n",
    "            req = self.session.get('{}{}'.format(self.url_base, url), headers=self.headers)\n",
    "        except:\n",
    "            print(\"Oops!\", sys.exc_info()[0], \"occured.\")\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    def get_publication_list(self, bs):\n",
    "        '''\n",
    "        Publication list from banxico.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        url: website url\n",
    "        '''\n",
    "        table = bs.find('table', {'class': {'table table-striped bmtableview'}})\n",
    "        if table == None:\n",
    "            print(\"Table not found.\")\n",
    "            return None\n",
    "\n",
    "        tr_list = table.find('tbody').findChildren('tr')\n",
    "        if tr_list == None:\n",
    "            print(\"Items into table not found\")\n",
    "            return None\n",
    "\n",
    "        corpus = []\n",
    "\n",
    "        for tr in tr_list:\n",
    "            date_view = tr.find('td', {'class': {'bmdateview'}})\n",
    "            text_view = tr.find('td', {'class': {'bmtextview'}})\n",
    "            url = tr.find('a')\n",
    "            label = None\n",
    "\n",
    "            if date_view != None:\n",
    "                date_view = date_view.get_text().strip()\n",
    "            if text_view != None:\n",
    "                text_view = text_view.get_text().split('Texto completo')[0].strip()\n",
    "                if 'incrementa' in text_view.lower() or 'aumenta' in text_view.lower():\n",
    "                    label = 'incrementa'\n",
    "                elif 'mantiene' in text_view.lower():\n",
    "                    label = 'mantiene'\n",
    "                elif 'disminuye' in text_view.lower() or 'reduce' in text_view.lower():\n",
    "                    label = 'disminuye'\n",
    "            if url != None and 'href' in url.attrs:\n",
    "                url = urllib.parse.urljoin(self.url_base, url.get('href'))\n",
    "            \n",
    "            publication = Publication(date_view, text_view, label, url, self.read_PDF(url), None, None)\n",
    "            #publication.__str__()\n",
    "            corpus.append(publication)\n",
    "    \n",
    "        return corpus\n",
    "    \n",
    "    def read_PDF(self, file_url):\n",
    "        '''\n",
    "        To read text from a pdf file\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        file_url: file url\n",
    "        '''\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        retstr = StringIO()\n",
    "        laparams = LAParams()\n",
    "        device = TextConverter(rsrcmgr, retstr, codec='utf-8', laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        \n",
    "        # Extract text\n",
    "        file = urllib.request.urlopen(file_url).read()\n",
    "        fp = BytesIO(file)\n",
    "        for page in PDFPage.get_pages(fp):\n",
    "            interpreter.process_page(page)\n",
    "        text = retstr.getvalue()\n",
    "        \n",
    "        # Cleanup\n",
    "        fp.close()\n",
    "        device.close()\n",
    "        retstr.close()\n",
    "        \n",
    "        if text != None:\n",
    "            text = text.strip()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def __str__(self):\n",
    "        '''\n",
    "        To string method\n",
    "        '''\n",
    "        print('session: {0}'.format(self.session))\n",
    "        print('url_base: {0}'.format(self.url_base))\n",
    "        print('headers: {0}'.format(self.headers))\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceso para obtener información de una página web. El proceso obtendrá el listado de las publicaciones de banxico desde https://www.banxico.org.mx/publicaciones-y-prensa/anuncios-de-las-decisiones-de-politica-monetaria/anuncios-politica-monetaria-t.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de publicaciones: 183\n",
      "\n",
      "Ejemplo, corpus[0]\n",
      "\n",
      "date_view: 19/12/19\n",
      "text_view: El objetivo para la Tasa de Interés Interbancaria a 1 día (tasa objetivo) disminuye en 25 puntos base\n",
      "label: disminuye\n",
      "url: https://www.banxico.org.mx/publicaciones-y-prensa/anuncios-de-las-decisiones-de-politica-monetaria/{D5328504-B958-4BE0-44A3-9363FCAC99D3}.pdf\n",
      "text: Anuncio de Política Mone...\n",
      "normalized_text: None\n",
      "token_list: None\n",
      "\n",
      "CPU times: user 42.8 s, sys: 775 ms, total: 43.6 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "crawler = Crawler()\n",
    "path = ('/publicaciones-y-prensa/'\n",
    "        'anuncios-de-las-decisiones-de-politica-monetaria/'\n",
    "        'anuncios-politica-monetaria-t.html')\n",
    "\n",
    "bs = crawler.get_page(path)\n",
    "publications = None\n",
    "\n",
    "if bs is not None:\n",
    "    #print(bs.prettify())\n",
    "    publications = crawler.get_publication_list(bs)\n",
    "    print('Número de publicaciones: {0}\\n'.format(len(publications)))\n",
    "    print('Ejemplo, corpus[0]\\n')\n",
    "    print(publications[0])\n",
    "else:\n",
    "    print('** No se pudo obtener listado de publicaciones.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Understanding Text \n",
    "\n",
    "### Text Tokenization\n",
    "\n",
    "- Sentence Tokenization (checked)\n",
    "- Word Tokenization (checked)\n",
    "\n",
    "### Text Normalization\n",
    "\n",
    "- Cleaning Text (checked)\n",
    "- Tokenizing Text (checked)\n",
    "- Removing Special Characters (checked)\n",
    "- Expanding Contractions (unnecessary)\n",
    "- Case Conversions (checked)\n",
    "- Removing Stopwords (checked)\n",
    "- Stemming\n",
    "- Lemmatization (checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    '''\n",
    "    Normalize class contains the main processes to normalize a text from a document.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    stopword_list: spanish stopword list from nltk module\n",
    "    wnl: WordNetLemmatizer instancce\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__()\n",
    "    tokenize_text(text)\n",
    "    normalize_corpus(text)\n",
    "    remove_special_characters_before(text)\n",
    "    remove_special_characters_after(text)\n",
    "    remove_stopwords(text)\n",
    "    remove_repeated_chars(text)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Constructor method\n",
    "        '''\n",
    "        self.stopword_list = nltk.corpus.stopwords.words('spanish')\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        \n",
    "    def tokenize_text(self, text):\n",
    "        '''\n",
    "        Method to tokenize a text from a document.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text: text from a document\n",
    "        '''\n",
    "        tokens = nltk.word_tokenize(text) \n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    def normalize_corpus(self, text):\n",
    "        '''\n",
    "        Method to normalize a text from a document.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text: text from a document\n",
    "        '''\n",
    "        text = text.lower()\n",
    "        text = self.remove_repeated_chars(text)\n",
    "        text = self.remove_special_characters_before(text)\n",
    "        text = self.remove_special_characters_after(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        return text\n",
    "    \n",
    "    def remove_special_characters_before(self, text):\n",
    "        '''\n",
    "        Method to remove general characters into a text from a document.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text: text from a document\n",
    "        '''\n",
    "        #PATTERN = r'[?|,|;|:|.|•|$|%|&|*|@|(|)|~]'\n",
    "        PATTERN = r'[•|~|ª|!|\"|·|$|%|&|/|(|)|=|?|¿|*|^|Ç|¨|_|:|;|º|\\'|¡|`|+|ç|´|-|.|,|\\\\|\\||@|#|¢|∞|¬|÷|“|≠|´|‚||}|{|–|…|„|}]'\n",
    "        #PATTERN = r'[^a-zA-Z0-9 ]'\n",
    "        tokens = self.tokenize_text(text)\n",
    "        filtered_tokens = filter(None, [re.sub(PATTERN, r'', token) for token in tokens])\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        return filtered_text\n",
    "\n",
    "    def remove_special_characters_after(self, text):\n",
    "        '''\n",
    "        Method to remove special characters into a text from a document.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text: text from a document\n",
    "        '''\n",
    "        tokens = self.tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "        filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        return filtered_text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        '''\n",
    "        Method to remove stopwords into a text from a document.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text: text from a document\n",
    "        '''\n",
    "        tokens = self.tokenize_text(text)\n",
    "        filtered_tokens = [token for token in tokens if token not in self.stopword_list]\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        return filtered_text\n",
    "    \n",
    "    def remove_repeated_chars(self, text):\n",
    "        '''\n",
    "        Method to remove repeated characters into a text from a document.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text: text from a document\n",
    "        '''\n",
    "        tokens = self.tokenize_text(text)\n",
    "        repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        match_substitution = r'\\1\\2\\3'\n",
    "\n",
    "        def replace(old_word):\n",
    "            if wn.synsets(old_word):\n",
    "                return old_word\n",
    "            new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "            return replace(new_word) if new_word != old_word else new_word\n",
    "\n",
    "        correct_tokens = [replace(word) for word in tokens]\n",
    "        correct_tokens = ' '.join(correct_tokens)\n",
    "        \n",
    "        return correct_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceso para obtener listado de documentos y listado de etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:67: FutureWarning: Possible set union at position 91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palabras promedio por documento: 548.6174863387978\n",
      "\n",
      "Ejemplo, corpus[0].token_list[:24]\n",
      "\n",
      "['anuncio', 'política', 'monetaria', 'comunicado', 'prensa', '19', 'diciembre', '2019', 'junta', 'gobierno', 'banco', 'méxico', 'decidido', 'disminuir', '25', 'puntos', 'base', 'objetivo', 'tasa', 'interés', 'interbancaria', 'día', 'nivel', '725']\n",
      "\n",
      "Etiquetas, labels:\n",
      "\n",
      "{'disminuye', 'incrementa', 'mantiene'}\n",
      "\n",
      "CPU times: user 7.22 s, sys: 131 ms, total: 7.35 s\n",
      "Wall time: 7.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mean_word = []\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(publications)):\n",
    "    normalize = Normalize()\n",
    "    if publications[i].text != None:\n",
    "        publications[i].normilized_text = normalize.normalize_corpus(publications[i].text)\n",
    "        publications[i].token_list = normalize.tokenize_text(publications[i].normilized_text)\n",
    "        corpus.append(publications[i].normilized_text)\n",
    "        labels.append(publications[i].label)\n",
    "        mean_word.append(len(publications[i].token_list))\n",
    "    else:\n",
    "        print('Publication without text content')\n",
    "    \n",
    "print('Número de palabras promedio por documento: {}\\n'.format(np.mean(mean_word)))\n",
    "print('Ejemplo, corpus[0].token_list[:24]\\n\\n{}\\n'.format(publications[0].token_list[:24]))\n",
    "print('Etiquetas, labels:\\n\\n{}\\n'.format(set(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction (Feature Engineering)\n",
    "\n",
    "### Feature-extraction techniques:\n",
    "- Bag of Words model\n",
    "- TF-IDF model\n",
    "- Advanced word vectorization models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction:\n",
    "    \n",
    "    def bow_extractor(self, corpus, ngram_range=(1,1)):\n",
    "        '''\n",
    "        Bag of Words model converts text documents into vectors such that each document is \n",
    "        converted into a vector that represents the frequency of all the distinct words that \n",
    "        are present in the document vector space for that specific document.\n",
    "        \n",
    "        CountVectorizer class converts a collection of text documents to a matrix of token \n",
    "        counts. If you do not provide an a-priori dictionary and you do not use an analyzer \n",
    "        that does some kind of feature selection then the number of features will be equal to \n",
    "        the vocabulary size found by analyzing the data.\n",
    "        \n",
    "        Important notes\n",
    "        ------------------\n",
    "        min_df=1 => indicates taking terms having a minimum frequency of 1 in the overall \n",
    "        document vector space.\n",
    "        ngram_range => has various parameters, feature vectors, like (1, 3) consisting of all \n",
    "        unigrams, bigrams, and trigrams.\n",
    "        fit_transform(self, raw_documents[, y]) => Learn the vocabulary dictionary and return \n",
    "        term-document matrix.\n",
    "        \n",
    "        More information \n",
    "        ------------------\n",
    "        CountVectorizer class => https://scikit-learn.org/stable/search.html?q=CountVectorizer\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        corpus: text to convert\n",
    "        ngram_range: to take into account n-grams as features\n",
    "        '''\n",
    "        vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "        features = vectorizer.fit_transform(corpus)\n",
    "        return vectorizer, features\n",
    "    \n",
    "    def tfidf_extractor(self, corpus, ngram_range=(1,1)):\n",
    "        '''\n",
    "        Term Frequency-Inverse Document Frequency (TF-IDF) is a combination of two metrics: \n",
    "        term frequency and inverse document frequency. Term frequency denoted by tf is what \n",
    "        we had computed in the Bag of Words model. It is denoted by the raw frequency value \n",
    "        of that term in a particular document. Inverse document frequency denoted by idf is \n",
    "        the inverse of the document frequency for each term. It is computed by dividing the \n",
    "        total number of documents in our corpus by the document frequency for each term and \n",
    "        then applying logarithmic scaling on the result.\n",
    "        \n",
    "        Important notes\n",
    "        ------------------\n",
    "        norm='l2' => smoothens the idfs to give weightages also to terms that may have zero \n",
    "        idf so that we do not ignore them.\n",
    "        ngram_range => has various parameters, feature vectors, like (1, 3) consisting of all \n",
    "        unigrams, bigrams, and trigrams.\n",
    "        fit_transform(self, raw_documents[, y]) => Learn the vocabulary dictionary and return \n",
    "        term-document matrix.\n",
    "        \n",
    "        More information \n",
    "        ------------------\n",
    "        CountVectorizer class => https://scikit-learn.org/stable/search.html?q=TfidfVectorizer\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        corpus: text to convert\n",
    "        ngram_range: to take into account n-grams as features\n",
    "        '''\n",
    "        vectorizer = TfidfVectorizer(min_df=1, norm='l2', smooth_idf=True, use_idf=True, ngram_range=ngram_range)\n",
    "        features = vectorizer.fit_transform(corpus)\n",
    "        return vectorizer, features\n",
    "    \n",
    "    def average_word_vectors(self, words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model[word])\n",
    "                if nwords:\n",
    "                    feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "        return feature_vector\n",
    "    \n",
    "    def averaged_word_vectorizer(self, corpus, model, num_features):\n",
    "        vocabulary = set(model.wv.index2word)\n",
    "        features = [self.average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "        return np.array(features)\n",
    "    \n",
    "    def tfidf_wtd_avg_word_vectors(self, words, tfidf_vector, tfidf_vocabulary, model, num_features):\n",
    "        word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)] \n",
    "                       if tfidf_vocabulary.get(word) else 0 for word in words]    \n",
    "        word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}\n",
    "        feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "        vocabulary = set(model.wv.index2word)\n",
    "        wts = 0.\n",
    "    \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                word_vector = model[word]\n",
    "                weighted_word_vector = word_tfidf_map[word] * word_vector\n",
    "                wts = wts + word_tfidf_map[word]\n",
    "                feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "                \n",
    "        if wts:\n",
    "            feature_vector = np.divide(feature_vector, wts)\n",
    "\n",
    "        return feature_vector\n",
    "    \n",
    "    def tfidf_weighted_averaged_word_vectorizer(self, corpus, tfidf_vectors, tfidf_vocabulary, model, num_features):\n",
    "        docs_tfidfs = [(doc, doc_tfidf) for doc, doc_tfidf in zip(corpus, tfidf_vectors)]\n",
    "        features = [self.tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary, model, num_features)\n",
    "                    for tokenized_sentence, tfidf in docs_tfidfs]\n",
    "        return np.array(features)\n",
    "    \n",
    "    def prepare_datasets(self, corpus, labels, test_data_proportion=0.3):\n",
    "        train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "            corpus, labels, test_size=0.33, random_state=42)\n",
    "        return train_X, test_X, train_Y, test_Y\n",
    "    \n",
    "    def show_features(self, features, feature_names):\n",
    "        return pd.DataFrame(data=features, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureExt = FeatureExtraction()\n",
    "\n",
    "train_corpus, test_corpus, train_labels, test_labels = featureExt.prepare_datasets(\n",
    "    corpus, labels, test_data_proportion=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words Model, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer, bow_train_features = featureExt.bow_extractor(train_corpus)\n",
    "bow_test_features = bow_vectorizer.transform(test_corpus)\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "#print(featureExt.show_features(bow_train_features, feature_names).head())\n",
    "#print(featureExt.show_features(bow_test_features, feature_names).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF) Model, samples\n",
    "\n",
    "$$ idf(t)=1 + \\log \\frac{C}{1+df(t)} $$\n",
    "\n",
    "$$ tfidf = \\frac{tfidf}{\\left|\\left| tfidf\\ \\right|\\right|} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer, tfidf_train_features = featureExt.tfidf_extractor(train_corpus)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(test_corpus)\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "#print(featureExt.show_features(tfidf_train_features, feature_names).head())\n",
    "#print(featureExt.show_features(tfidf_test_features, feature_names).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Word Vectorization Models\n",
    "\n",
    "- Averaged word vectors\n",
    "- TF-IDF weighted word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize corpora (documents)\n",
    "tokenized_train = [nltk.word_tokenize(text) for text in train_corpus]\n",
    "tokenized_test = [nltk.word_tokenize(text) for text in test_corpus]\n",
    "\n",
    "# build the word2vec model on our training corpus\n",
    "model = gensim.models.Word2Vec(tokenized_train, size=500, window=100, min_count=30, sample=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged Word Vectors\n",
    "\n",
    "$$ AWV(D) = \\frac{{\\displaystyle {\\sum}_1^n}wv(w)}{n} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:75: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "avg_wv_train_features = featureExt.averaged_word_vectorizer(\n",
    "    corpus=tokenized_train, model=model, num_features=500)\n",
    "avg_wv_test_features = featureExt.averaged_word_vectorizer(\n",
    "    corpus=tokenized_test, model=model, num_features=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Weighted Averaged Word Vectors\n",
    "\n",
    "$$ TWA(D) = \\frac{{\\displaystyle {\\sum}_1^n}wv(w) \\times tfidf(w)}{n} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:97: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "\n",
    "tfidf_wv_train_features = featureExt.tfidf_weighted_averaged_word_vectorizer(\n",
    "    corpus=tokenized_train, tfidf_vectors=tfidf_train_features, tfidf_vocabulary=vocab, model=model, num_features=500)\n",
    "tfidf_wv_test_features = featureExt.tfidf_weighted_averaged_word_vectorizer(\n",
    "    corpus=tokenized_test, tfidf_vectors=tfidf_test_features, tfidf_vocabulary=vocab, model=model, num_features=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various types of classification algorithms:\n",
    "- Multinomial Naïve Bayes\n",
    "- Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification:\n",
    "\n",
    "    def prepare_datasets(self, corpus, labels, test_data_proportion=0.3):\n",
    "        train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels, test_size=0.33, random_state=42)\n",
    "        return train_X, test_X, train_Y, test_Y\n",
    "\n",
    "    def remove_empty_docs(self, corpus, labels):\n",
    "        filtered_corpus = []\n",
    "        filtered_labels = []\n",
    "        for doc, label in zip(corpus, labels):\n",
    "            if doc.strip():\n",
    "                filtered_corpus.append(doc)\n",
    "                filtered_labels.append(label)\n",
    "        return filtered_corpus, filtered_labels\n",
    "    \n",
    "    def train_predict_evaluate_model(self, model_name, classifier, train_features, train_labels, test_features, test_labels):\n",
    "        # build model    \n",
    "        classifier.fit(train_features, train_labels)\n",
    "        # predict using model\n",
    "        predictions = classifier.predict(test_features) \n",
    "        # evaluate model prediction performance   \n",
    "        self.get_metrics(model_name, true_labels=test_labels, predicted_labels=predictions)\n",
    "        return predictions \n",
    "    \n",
    "    def get_metrics(self, model_name, true_labels, predicted_labels):\n",
    "        print('******** {} ********'.format(model_name))\n",
    "        print('Accuracy:', np.round(metrics.accuracy_score(true_labels, predicted_labels), 2))\n",
    "        print('Precision:', np.round(metrics.precision_score(true_labels, predicted_labels, average='weighted'), 2))\n",
    "        print('Recall:', np.round(metrics.recall_score(true_labels, predicted_labels, average='weighted'), 2))\n",
    "        print('F1 Score:', np.round(metrics.f1_score(true_labels, predicted_labels, average='weighted'), 2))\n",
    "        print('Confusion matrix:\\n', self.get_confusion_matrix(true_labels, predicted_labels))\n",
    "        print('')\n",
    "        \n",
    "    def get_confusion_matrix(self, test_labels, predictions):\n",
    "        '''\n",
    "        Method to get the confusion matrix about result.\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        test_corpus: text from corpus\n",
    "        test_labels: labels from corpus\n",
    "        predictions: predicted labels\n",
    "        '''\n",
    "        cm = metrics.confusion_matrix(test_labels, predictions, labels=['disminuye','incrementa','mantiene'])\n",
    "        return pd.DataFrame(\n",
    "            data=cm, \n",
    "            columns=pd.MultiIndex(\n",
    "                levels=[['Predicted:'], ['disminuye','incrementa','mantiene']],\n",
    "                labels=[[0,0,0],[0,1,2]]),\n",
    "            index=pd.MultiIndex(\n",
    "                levels=[['Actual:'], ['disminuye','incrementa','mantiene']],\n",
    "                labels=[[0,0,0],[0,1,2]]))\n",
    "    \n",
    "    def get_fake_documents(self, test_corpus, test_labels, predictions):\n",
    "        '''\n",
    "        Method to information about the publications which were incorrectly predicted.\n",
    "        \n",
    "        Parameters\n",
    "        ------------------\n",
    "        test_corpus: text from corpus\n",
    "        test_labels: labels from corpus\n",
    "        predictions: predicted labels\n",
    "        '''\n",
    "        for document, label, predicted_label in zip(test_corpus, test_labels, predictions):\n",
    "            if label != predicted_label:\n",
    "                print('Actual label:', label)\n",
    "                print('Predicted label:', predicted_label)\n",
    "                print('Document text:')\n",
    "                print(re.sub('\\n', ' ', document[:200]))\n",
    "                print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** Multinomial Naive Bayes with bag of words features ********\n",
      "Accuracy: 0.74\n",
      "Precision: 0.77\n",
      "Recall: 0.74\n",
      "F1 Score: 0.69\n",
      "Confusion matrix:\n",
      "                    Predicted:                    \n",
      "                    disminuye incrementa mantiene\n",
      "Actual: disminuye           2          0        4\n",
      "        incrementa          0          4       11\n",
      "        mantiene            0          1       39\n",
      "\n",
      "******** Support Vector Machine with bag of words features ********\n",
      "Accuracy: 0.8\n",
      "Precision: 0.82\n",
      "Recall: 0.8\n",
      "F1 Score: 0.78\n",
      "Confusion matrix:\n",
      "                    Predicted:                    \n",
      "                    disminuye incrementa mantiene\n",
      "Actual: disminuye           2          1        3\n",
      "        incrementa          0          8        7\n",
      "        mantiene            0          1       39\n",
      "\n",
      "******** Multinomial Naive Bayes with TF-IDF features ********\n",
      "Accuracy: 0.66\n",
      "Precision: 0.43\n",
      "Recall: 0.66\n",
      "F1 Score: 0.52\n",
      "Confusion matrix:\n",
      "                    Predicted:                    \n",
      "                    disminuye incrementa mantiene\n",
      "Actual: disminuye           0          0        6\n",
      "        incrementa          0          0       15\n",
      "        mantiene            0          0       40\n",
      "\n",
      "******** Support Vector Machine with TF-IDF features ********\n",
      "Accuracy: 0.8\n",
      "Precision: 0.82\n",
      "Recall: 0.8\n",
      "F1 Score: 0.78\n",
      "Confusion matrix:\n",
      "                    Predicted:                    \n",
      "                    disminuye incrementa mantiene\n",
      "Actual: disminuye           2          1        3\n",
      "        incrementa          0          8        7\n",
      "        mantiene            0          1       39\n",
      "\n",
      "******** Support Vector Machine with averaged word vector features ********\n",
      "Accuracy: 0.25\n",
      "Precision: 0.06\n",
      "Recall: 0.25\n",
      "F1 Score: 0.1\n",
      "Confusion matrix:\n",
      "                    Predicted:                    \n",
      "                    disminuye incrementa mantiene\n",
      "Actual: disminuye           0          6        0\n",
      "        incrementa          0         15        0\n",
      "        mantiene            0         40        0\n",
      "\n",
      "******** Support Vector Machine with tfidf weighted averaged word vector features ********\n",
      "Accuracy: 0.69\n",
      "Precision: 0.6\n",
      "Recall: 0.69\n",
      "F1 Score: 0.61\n",
      "Confusion matrix:\n",
      "                    Predicted:                    \n",
      "                    disminuye incrementa mantiene\n",
      "Actual: disminuye           0          1        5\n",
      "        incrementa          0          3       12\n",
      "        mantiene            0          1       39\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n"
     ]
    }
   ],
   "source": [
    "classification = Classification()\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge')\n",
    "\n",
    "## Bag of words features\n",
    "\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "mnb_bow_predictions = classification.train_predict_evaluate_model(\n",
    "    model_name='Multinomial Naive Bayes with bag of words features',\n",
    "    classifier=mnb, train_features=bow_train_features, train_labels=train_labels, \n",
    "    test_features=bow_test_features, test_labels=test_labels)\n",
    "# Support Vector Machine with bag of words features\n",
    "svm_bow_predictions = classification.train_predict_evaluate_model(\n",
    "    model_name='Support Vector Machine with bag of words features',\n",
    "    classifier=svm, train_features=bow_train_features, train_labels=train_labels, \n",
    "    test_features=bow_test_features, test_labels=test_labels)\n",
    "\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF) features\n",
    "\n",
    "# Multinomial Naive Bayes with TF-IDF features\n",
    "mnb_tfidf_predictions = classification.train_predict_evaluate_model(\n",
    "    model_name='Multinomial Naive Bayes with TF-IDF features',\n",
    "    classifier=mnb, train_features=tfidf_train_features, train_labels=train_labels,\n",
    "    test_features=tfidf_test_features, test_labels=test_labels)\n",
    "# Support Vector Machine with TF-IDF features\n",
    "svm_tfidf_predictions = classification.train_predict_evaluate_model(\n",
    "    model_name='Support Vector Machine with TF-IDF features',\n",
    "    classifier=svm, train_features=tfidf_train_features, train_labels=train_labels,\n",
    "    test_features=tfidf_test_features, test_labels=test_labels)\n",
    "\n",
    "\n",
    "## Averaged Word Vector features\n",
    "\n",
    "# Support Vector Machine with averaged word vector features\n",
    "svm_avgwv_predictions = classification.train_predict_evaluate_model(\n",
    "    model_name='Support Vector Machine with averaged word vector features',\n",
    "    classifier=svm, train_features=avg_wv_train_features, train_labels=train_labels,\n",
    "    test_features=avg_wv_test_features, test_labels=test_labels)\n",
    "# Support Vector Machine with tfidf weighted averaged word vector features\n",
    "svm_tfidfwv_predictions = classification.train_predict_evaluate_model(\n",
    "    model_name='Support Vector Machine with tfidf weighted averaged word vector features',\n",
    "    classifier=svm, train_features=tfidf_wv_train_features, train_labels=train_labels,\n",
    "    test_features=tfidf_wv_test_features, test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: incrementa\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "comunicado prensa 23 julio 204 anuncio política monetaria junta gobierno banco méxico decidido aumentar corto ” 41 milones pesos partir hoy mantiene expansión económica mundial si bien ritmo ligeramen\n",
      "\n",
      "Actual label: incrementa\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "anuncio política monetaria comunicado prensa 8 febrero 2018 junta gobierno banco méxico decidido aumentar 25 puntos base objetivo tasa interés interbancaria día nivel 750 ciento economía mundial sigui\n",
      "\n",
      "Actual label: incrementa\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "15 agosto 208 comunicado prensa anuncio política monetaria junta gobierno banco méxico decidido incrementar 825 ciento objetivo tasa interés interbancaria 1 día desaceleración económica mundial intens\n",
      "\n",
      "Actual label: disminuye\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "6 junio 2014 anuncio política monetaria comunicado prensa junta gobierno banco méxico decidido disminuir 50 puntos base objetivo tasa interés interbancaria día nivel 30 ciento crecimiento economía mun\n",
      "\n",
      "Actual label: incrementa\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "14 diciembre 2017 anuncio política monetaria comunicado prensa junta gobierno banco méxico decidido aumentar 25 puntos base objetivo tasa interés interbancaria día nivel 725 ciento economía mundial co\n",
      "\n",
      "Actual label: disminuye\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "6 septiembre 2013 anuncio política monetaria comunicado prensa junta gobierno banco méxico decidido disminuir 25 puntos base objetivo tasa interés interbancaria día nivel 375 ciento economía mundial m\n",
      "\n",
      "Actual label: incrementa\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "comunicado prensa 24 septiembre 204 anuncio política monetaria junta gobierno banco méxico decidido aumentar corto ” 51 milones pesos partir hoy prevé continúe expansión actividad económica mundial au\n",
      "\n",
      "Actual label: incrementa\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "anuncio política monetaria comunicado prensa 15 noviembre 2018 junta gobierno banco méxico decidido incrementar 25 puntos base objetivo tasa interés interbancaria día nivel 80 tercer trimestre 2018 ec\n",
      "\n",
      "Actual label: incrementa\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "comunicado prensa 27 abril 204 comunicado prensa pasado 23 abril banco méxico anunciar dejaba cambio monto corto ” 33 milones pesos señaló siguiente considerando inflación aún encuentra niveles elevad\n",
      "\n",
      "Actual label: disminuye\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "8 marzo 2013 anuncio política monetaria comunicado prensa junta gobierno banco méxico decidido disminuir 50 puntos base objetivo tasa interés interbancaria día nivel 40 ciento economía mundial continú\n",
      "\n",
      "Actual label: incrementa\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "comunicado prensa 27 agosto 204 anuncio política monetaria junta gobierno banco méxico decidido aumentar corto ” 45 milones pesos partir hoy expansión económica mundial mantiene si bien ritmo inferior\n",
      "\n",
      "Actual label: disminuye\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "fecha 31 julio 201 número 81 c m u n i c c i ó n s c i l b l t í n d p r n s d u r n t l p r i d n r j u n i d l p r s n t ñ l c r c i m i n t c u m u l d d l i n p c f u d 2 1 1 p r c i n t l i n f l\n",
      "\n",
      "Actual label: incrementa\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "anuncio política monetaria comunicado prensa 21 junio 2018 junta gobierno banco méxico decidido incrementar 25 puntos base objetivo tasa interés interbancaria día nivel 775 indicadores disponibles sug\n",
      "\n",
      "Actual label: incrementa\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "actividad económica comunicado prensa 20 febrero 204 recuperación cíclica economía global continúa consolidándose observado mayor claridad unidos información estadística reciente propició reserva fede\n",
      "\n",
      "Actual label: mantiene\n",
      "Predicted label: incrementa\n",
      "Document text:\n",
      "comunicado prensa trayectoria reciente principales variables económicas financieras 21 febrero 203 pasado reciente actividad económica méxico caracterizado cierto debilitamiento recuperación inició ha\n",
      "\n",
      "Actual label: incrementa\n",
      "Predicted label: mantiene\n",
      "Document text:\n",
      "29 septiembre 2016 anuncio política monetaria comunicado prensa junta gobierno banco méxico decidido aumentar 50 puntos base objetivo tasa interés interbancaria día nivel 475 ciento perspectivas creci\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification.get_fake_documents(test_corpus, test_labels, mnb_bow_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classification Models\n",
    "\n",
    "Metrics:\n",
    "\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "\n",
    "A $confusion$ $matrixis$ a tabular structure that helps visualize the performance of classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='margin-bottom:20px;'>Data Science Module</h1>\n",
    "<p style='margin:0;padding:0;'>Sergio de la Cruz Badillo</p>\n",
    "<p style='margin:0;padding:0;'>dlcruzser12@gmail.com</p>\n",
    "<p style='margin:0;padding:0;'>06/Enero/2020</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
