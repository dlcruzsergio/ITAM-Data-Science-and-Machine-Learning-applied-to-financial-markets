{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sergio de la Cruz Badillo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descomposición por valores singulares y sistemas de recomendación.\n",
    "\n",
    "Al igual que el análisis de componentes principales (PCA por sus siglas en inglés), la descomposición por valores singulares (SVD en inglés) es uno de los métodos más usuales para reducir la dimensión de nuestro conjunto de datos.\n",
    "\n",
    "A diferencia del PCA, este método no requiere del uso de la matriz de **varianzas y covarianzas**.\n",
    "\n",
    "+ **A favor:** Reduce la dimensión de nuestros datos, remueve el ruido que podrían contener.\n",
    "+ **En contra:** Se pierde interpretación en los datos transformados.\n",
    "+ **Tipo de datos:** Numéricos.\n",
    "\n",
    "En estas notas, se expondrá la relación entre SVD y los sistemas de recomendación.\n",
    "***\n",
    "\n",
    "## Descomposición de la matriz de datos.\n",
    "Sea $\\mathbf{D_{m \\times n}}$ una matriz de $m \\times n$ la cual representa nuestro conjunto de datos. El método de descomposición por valores singulares nos dice que:\n",
    "\n",
    "$$ \\mathbf{D_{m \\times n}} = \\mathbf{U_{m \\times m}} \\mathbf{\\Sigma_{m \\times n}} \\mathbf{V_{n \\times n}^{T}}$$\n",
    "\n",
    "en donde la matriz $\\mathbf{\\Sigma_{m \\times n}}$ es una matriz con elementos igual a $0$, excepto posiblemente, en su diagonal. Estos valores distintos de cero, reciben el nombre de **valores singulares** y son igual a la raiz cuadrada de los eigenvalores de la matriz $\\mathbf{D_{m \\times n}} \\mathbf{D_{m \\times n}^{T}}$. Además, los valores en la diagonal están ordenados de mayor a menor.\n",
    "\n",
    "## SVD en Python con numpy.\n",
    "Es posible obtener factorización anterior utilizando el paquete **numpy**, en particular, la función **svd** del módulo **linalg**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de U:\n",
      "[[-0.3863177  -0.92236578]\n",
      " [-0.92236578  0.3863177 ]]\n",
      "\n",
      "Valores de sig:\n",
      "[9.508032   0.77286964]\n",
      "\n",
      "Valores de VT:\n",
      "[[-0.42866713 -0.56630692 -0.7039467 ]\n",
      " [ 0.80596391  0.11238241 -0.58119908]\n",
      " [ 0.40824829 -0.81649658  0.40824829]]\n",
      "\n",
      "m es igual a 2\n",
      "n es igual a 3\n",
      "La dimensión de U es (2, 2)\n",
      "La dimensión de sigma es (2,)\n",
      "La dimensión de V^T es (3, 3)\n",
      "[9.508032   0.77286964]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "datos = np.matrix([[1,2,3], [4,5,6]])\n",
    "num_renglones = datos.shape[0] # m\n",
    "num_columnas = datos.shape[1] #n\n",
    "U, sig, VT = np.linalg.svd(datos)\n",
    "\n",
    "print(f\"Valores de U:\\n{U}\", end=\"\\n\\n\")\n",
    "print(f\"Valores de sig:\\n{sig}\", end=\"\\n\\n\")\n",
    "print(f\"Valores de VT:\\n{VT}\", end=\"\\n\\n\")\n",
    "\n",
    "print('m es igual a ' + str(num_renglones))\n",
    "print('n es igual a ' + str(num_columnas))\n",
    "print('La dimensión de U es ' + str(U.shape))\n",
    "print('La dimensión de sigma es ' + str(sig.shape))\n",
    "print('La dimensión de V^T es ' + str(VT.shape))\n",
    "print(sig) #No es una matriz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, la función **svd**, en lugar de regresar la matriz $\\mathbf{\\Sigma_{m \\times n}}$, nos regresa un vector que representa la diagonal de esta matriz. Esto se hace por cuestiones de ahorro en la memoria, recuerde que el resto de los elementos es igual a cero.\n",
    "\n",
    "Para poder obtener $\\mathbf{\\Sigma_{m \\times n}}$, utilizaremos las funciones **zeros** y **fill_diagonal** de **numpy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crea_matriz_sigma(sig, num_renglones, num_columnas, aprox = False):\n",
    "    '''\n",
    "    ENTRADA:\n",
    "    sig: Arreglo con los valores singulares\n",
    "    \n",
    "    num_renglones: Entero que representa el número de renglones\n",
    "    \n",
    "    num_columnas: Entero que representa el número de columnas\n",
    "    \n",
    "    aprox: Booleano, si True se crea una matriz cuadrada sigma que aproxima a nuestra matriz de datos.\n",
    "    En este caso sólo se utiliza un subconjunto de los valores singulares.\n",
    "    Si False, se crea una matriz rectangular utilizando todos los valores singulares\n",
    "    \n",
    "    SALIDA:\n",
    "    matriz con la diagonal formada por sig\n",
    "    '''\n",
    "    if aprox:\n",
    "        mat_sigma = np.zeros((num_renglones, num_renglones)) #matriz cuadrangular de ceros\n",
    "    else:\n",
    "        mat_sigma = np.zeros((num_renglones, num_columnas)) #matriz rectangular de ceros\n",
    "    np.fill_diagonal(mat_sigma, sig) #Se modifica la diagonal\n",
    "    \n",
    "    return mat_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de matriz sigma:\n",
      "[[9.508032   0.         0.        ]\n",
      " [0.         0.77286964 0.        ]]\n",
      "\n",
      "Tamaño de matriz sigma:\n",
      "(2, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mat_sigma = crea_matriz_sigma(sig, num_renglones, num_columnas, False)\n",
    "print(f\"Valores de matriz sigma:\\n{mat_sigma}\", end=\"\\n\\n\")\n",
    "print(f\"Tamaño de matriz sigma:\\n{mat_sigma.shape}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifiquemos que la descomposición fue correcta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matriz original es:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "El producto de la descomposición es:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"La matriz original es:\\n{datos}\", end=\"\\n\\n\")\n",
    "print(f\"El producto de la descomposición es:\\n{U * mat_sigma * VT}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de la dimensión.\n",
    "Algo que se ha observado en la práctica, es el hecho de que sólo un número, $k$, de los valores en la diagonal de la matriz $\\mathbf{\\Sigma_{m \\times n}}$ son distintos de cero, esto indica que sólo un conjunto de $k$ atributos son considerados como importantes, los demás son ruido o repeticiones.\n",
    "\n",
    "Así pues, podemos aproximar nuestro conjunto de datos utilizando sólomente una porción de la matriz $\\mathbf{\\Sigma_{m \\times n}}$.\n",
    "\n",
    "Una forma de elegir el número de valores singulares a utilizar, es estableciendo un umbral (e.g. $90\\%$) para la \"energía\" explicada por estos valores. Esto se puede obtener sumando los cuadrados de cada valor en la diagonal (\"energía\" total) y considerar (agregando de mayor a menor valor) los $k$ valores singulares cuya \"energía\" agregada represente al menos cierto porcentaje de la energía total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecciona_k_importantes(sig, umbral = 0.90):\n",
    "    '''\n",
    "    ENTRADA:\n",
    "    sig: Arreglo con los valores singulares (deben de estar ordenados de mayor a menor)\n",
    "    \n",
    "    umbral: Número en (0,1) que representa la cantidad mínima de \"energía\"\n",
    "    explicada por los valores singulares\n",
    "    \n",
    "    SALIDA:\n",
    "    Un arreglo con los k valores singulares más importantes\n",
    "    '''\n",
    "    \n",
    "    #Calcula la energía total\n",
    "    total = np.sum(sig**2.0)\n",
    "    \n",
    "    #Calcula las proporciones acumuladas (ordenadas de mayor a menor)\n",
    "    proporciones = np.cumsum(sig**2.0) / total\n",
    "    \n",
    "    #para almacenar los k mejores\n",
    "    k_mejores = []\n",
    "    \n",
    "    for k in range(0, len(proporciones)):\n",
    "        if proporciones[k] < umbral:\n",
    "            k_mejores.append(sig[k])\n",
    "        #Ya que las proporciones acumuladas se ordenan de mayor a menor\n",
    "        #se sale del cíclo el primer momento en que se rebasa el umbral\n",
    "        else:\n",
    "            #Este k-ésimo elemento es el primero con el cual se rebasa el umbral\n",
    "            k_mejores.append(sig[k]) \n",
    "            break\n",
    "    #se convierte la lista en un arreglo de numpy\n",
    "    k_mejores = np.array(k_mejores)\n",
    "    \n",
    "    return k_mejores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sistemas de recomendación basados en filtrado colaborativo.\n",
    "\n",
    "¿Alguna vez se ha preguntado como Netflix, Amazon, Youtube, etc. Son capaces de realizar recomendaciones que coinciden con nuestros intereses?\n",
    "\n",
    "Un sistema de recomendación busca predecir la valoración o preferencia que un usuario le daría a un artículo determinado (por artículo podemos referirnos a películas, canciones, libros, series accionarias, etc.)\n",
    "\n",
    "Nosotros nos enfocaremos en sistemas de recomendación basados en filtrado colaborativo, estos sistemas funcionan comparando las opiniones o preferencias de los usuarios hacia ciertos artículos.\n",
    "\n",
    "El filtrado colaborativo se basa en la hipótesis de que si dos usuarios tienen preferencias similares respecto a un conjunto de artículos, entonces su preferencia coincidirá para un artículo distinto. Es entonces necesario introducir el concepto de similitud.\n",
    "\n",
    "### Funciones de similitud.\n",
    "Sea $\\mathbf{X}$ un conjunto. Una función $s:\\mathbf{X} \\times \\mathbf{X} \\rightarrow \\mathbb{R}$ se llama una función de similitud si:\n",
    "\n",
    "+ Es no negativa: $s(x,y) \\geq 0$, para todo $x, y \\in \\mathbf{X}$.\n",
    "+ Es simétrica: $ s(x,y) = s(y,x)$.\n",
    "+ Se cumple que $s(x,y) \\leq s(x, x)$ para todo $x, y \\in \\mathbf{X}$. Con igualdad si y sólo si $x = y$.\n",
    "\n",
    "#### Ejemplos de funciones de similitud.\n",
    " Sean $\\mathbf{X} = \\mathbb{R}^n$, algunos ejemplos de funciones de similitud son los siguientes:\n",
    " \n",
    " + $cos(\\mathbf{x}, \\mathbf{y}) = 0.5 + 0.5 \\dfrac{ < \\mathbf{x}, \\mathbf{y} >} {||\\mathbf{x}||_{2}|| \\mathbf{y}||_{2}}$ Similitud coseno (normalizada entre 0 y 1)\n",
    "\n",
    "\n",
    " \n",
    " + $euclid(\\mathbf{x}, \\mathbf{y}) =  \\dfrac{1} { 1 + || \\mathbf{x} - \\mathbf{y} ||_2}$ Similitud euclideana\n",
    " \n",
    "### Representación de los datos.\n",
    "Usualmente, los datos se representan a través de una matriz en la cual los renglones representan los usuarios y las columnas los artículos (items).\n",
    "Aquí supondremos que nuestra matriz representa la opinión que tienen 100 analistas respecto a las 10 principales series accionarias de Índice de Precios y Cotizaciones [liga](https://espanol.spindices.com/indices/equity/sp-bmv-ipc).\n",
    "\n",
    "Cada analista emite una opinión respecto a cada una de las series accionarias:\n",
    "\n",
    "+ $0 \\implies$ todavía no se ha dado opinión.\n",
    "+ $1 \\implies$ opinión negativa.\n",
    "+ $2 \\implies$ opinión neutral.\n",
    "+ $3 \\implies$ opinión positiva.\n",
    "\n",
    "De esta manera, buscamos obtener una recomendación sobre cuál serie accionaria agregar a nuestro portafolio cuando todavía no tenemos una opinión para esta.\n",
    "\n",
    "Para simular esta matriz utilizamos la función **randint** del módulo **random** de **numpy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 2 0 0 2 2 2 0 3]\n",
      " [3 2 0 0 1 3 2 3 2 3]\n",
      " [0 2 1 3 1 3 1 2 0 2]\n",
      " [2 2 3 0 2 3 2 3 0 0]\n",
      " [3 0 2 2 1 0 2 1 1 1]\n",
      " [3 3 3 2 1 3 0 2 3 1]\n",
      " [1 3 0 3 2 2 1 2 0 2]\n",
      " [2 1 3 3 0 3 2 1 1 0]\n",
      " [2 0 1 2 1 2 1 1 2 3]\n",
      " [1 0 1 2 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#Diccionario auxiliar\n",
    "dicc_series = {0:'FEMSA', 1:'AMX', 2:'GFNORTE', 3:'WALMEX', 4:'GMEXICO', 5:'CEMEX', 6:'BIMBO',\n",
    "           7:'GAP', 8:'ELEKTRA', 9:'TELEVISA'}\n",
    "#Se fija semilla con fines de reproducir el experimento\n",
    "np.random.seed(54321)\n",
    "opiniones = np.random.randint(low = 0, high = 4, size = (100, 10))\n",
    "print(opiniones[0:10,]) #primeras diez opiniones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones de similitud\n",
    "def cos_sim(x, y):\n",
    "    '''\n",
    "    Calcula la similitud coseno normalizada entre 0 y 1\n",
    "    ENTRADA\n",
    "    x,y: Arreglos de numpy.\n",
    "    \n",
    "    SALIDA\n",
    "    similitud coseno entre x,y\n",
    "    '''\n",
    "    numerador = np.dot(x,y)\n",
    "    denominador = np.linalg.norm(x, ord = 2) * np.linalg.norm(y, ord = 2)\n",
    "    return 0.5 + 0.5 * (numerador / denominador)\n",
    "\n",
    "def euclid_sim(x,y):\n",
    "    '''\n",
    "    Calcula la similitud euclideana\n",
    "    ENTRADA\n",
    "    x,y: Arreglos de numpy.\n",
    "    \n",
    "    SALIDA\n",
    "    similitud euclideana entre x,y\n",
    "    '''\n",
    "    return 1.0 / (1 + np.linalg.norm(x - y))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se mencionó, el filtrado colaborativo se basa en el supuesto de que usuarios con preferencias similares para un conjunto de artículos, tendrán una preferencia similar para un nuevo artículo en consideración.\n",
    "\n",
    "Así, supongamos que el analista $\\mathbf{a_h}$ todavía no tiene una opinión para la serie accionaria $h$, en cambio, este mismo analista sí tiene una opinión para la serie accionaria $j$. Por otra parte, supongamos que existe un conjunto de analistas,  $\\mathbf{b_1}, \\ldots, \\mathbf{b_p}$, que tienen opiniones tanto para la serie $h$ como para las serie $j$.\n",
    "\n",
    "Con la información de estos analistas podemos crear los siguientes vectores:\n",
    "$$\n",
    "\\mathbf{r_{h}} = \n",
    "\\begin{pmatrix}\n",
    "r_{b_{1}h} \\\\\n",
    "\\vdots \\\\\n",
    "r_{b_{p}h} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{r_{j}} = \n",
    "\\begin{pmatrix}\n",
    "r_{b_{1}j} \\\\\n",
    "\\vdots \\\\\n",
    "r_{b_{p}j} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "en donde $r_{b_{i}h}$ representa la opinión que el analista $\\mathbf{b_i}$ tiene sobre la serie $h$, $r_{b_{i}j}$ se define de manera similar.\n",
    "\n",
    "Si $\\mathbf{r_{h}}$ y $\\mathbf{r_{j}}$ son similares (en nuestro caso esto quiere decir que la función de similitud es cercana a $1$), entonces los analistas $\\mathbf{b_1}, \\ldots, \\mathbf{b_p}$ tienen la misma opinión entre las series $h$ y $j$, por lo tanto, para el analista $\\mathbf{a_h}$, se debe asignar a la serie $h$ la misma opinión que asignó a la serie $j$ (se sigue a las multitudes).\n",
    "\n",
    "Ya que la serie $h$ se compara con varias series $j$, lo que realmente obtenemos es una opinión promedio y no un número en el conjunto $\\{0, 1, 2, 3\\}$.\n",
    "\n",
    "$$Opinion(\\mathbf{a_h},h) = \\dfrac{\\sum_{j \\in \\mathbf{A}} similitud(j,h) * opinion_{\\mathbf{a_h}}(j)}{\\sum_{j \\in \\mathbf{A}} similitud(j,h)}$$\n",
    "\n",
    "en donde\n",
    "\n",
    "+ $Opinion(\\mathbf{a_h},h)$ es la opinión del analista $\\mathbf{a_h}$ para la serie $h$.\n",
    "+ $similitud(j,h)$ es la similitud entre la serie $h$ y $j$, de acuerdo a los analistas que tienen opinión para ambas series.\n",
    "+ $opinion_{\\mathbf{a_h}}(j)$ es la opinión del analista $\\mathbf{a_h}$ para la serie $j$.\n",
    "+ $\\mathbf{A}$ es el conjunto de analistas $\\mathbf{b_1}, \\ldots, \\mathbf{b_p}$, es decir, aquellos analistas que tienen una opinión sobre las series $h$  y $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_opinion(opiniones, analista, fun_sim, serie_h):\n",
    "    '''\n",
    "    Función para calcular la opinión (promedio) que un analista tendría para una serie accionaria\n",
    "    \n",
    "    ENTRADA\n",
    "    opiniones: numpy.ndarray que representa las opiniones de un conjunto de analistas\n",
    "    \n",
    "    analista: Entero que representa el analista de interés (un índice de un renglón del arreglo opiniones)\n",
    "    \n",
    "    fun_sim: Función que se utilizará para calcular la similitud\n",
    "    \n",
    "    serie_h: Entero que representa la serie accionaria de interés (un índice de una columna del arreglo opiniones)\n",
    "    \n",
    "    SALIDA\n",
    "    Opinión promedio de analista para la serie\n",
    "    '''\n",
    "    \n",
    "    #Si el analista ya dio una opinión para la serie, no hay nada que hacer\n",
    "    if opiniones[analista, serie_h] != 0:\n",
    "        print('Para la serie ' + dicc_series[serie_h] + ' ya se tiene una opinión')\n",
    "        return opiniones[analista, serie_h]\n",
    "    \n",
    "    #número de series accionarias\n",
    "    n_series = opiniones.shape[1]\n",
    "    \n",
    "    #Para almacenar la similitud total y la similitud 'ponderada' por la opinión del analista\n",
    "    sim_total = 0.0; sim_pond = 0.0\n",
    "    \n",
    "    #Para cada serie accionaria j\n",
    "    for j in range(0, n_series):\n",
    "        #Opinión del analista para la serie j\n",
    "        opinion_serie = opiniones[analista, j]\n",
    "        \n",
    "        #Si el analista no tiene opinión para esta serie revisamos la siguiente\n",
    "        if opinion_serie == 0: continue\n",
    "        \n",
    "        #Se localiza a los analistas b1,...,bp que tienen opinión para la serie_h y la serie j\n",
    "        indices = np.nonzero(np.logical_and(opiniones[:, serie_h] > 0, opiniones[:, j] > 0))\n",
    "        \n",
    "        #Si nadie ha calificado la serie j, se establece una similitud de cero\n",
    "        if len(indices) == 0:\n",
    "            similitud = 0.0\n",
    "        else:\n",
    "            #Se calcula la similitud de acuerdo a la función fun_sim\n",
    "            #Los parámetros son los vectores rh y rj\n",
    "            #NOTA: opiniones[indices, serie_h] es un arreglo dentro de un arreglo, por eso se usa [0]\n",
    "            similitud = fun_sim(opiniones[indices, serie_h][0], opiniones[indices, j][0])\n",
    "        \n",
    "        #se acumulan y se ponderan las similitudes\n",
    "        sim_total = sim_total + similitud\n",
    "        sim_pond = sim_pond + similitud * opinion_serie\n",
    "        \n",
    "    #Si no hay ninguna serie j para comparar la serie_h entonces no hay opinión\n",
    "    if sim_total == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        #Opinión promedio\n",
    "        return sim_pond / sim_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud coseno:\n",
      "Para el analista 0 en la serie WALMEX se tiene 1.9997882689302027\n",
      "Similitud euclideana:\n",
      "Para el analista 0 en la serie WALMEX se tiene 1.9986945025331566\n"
     ]
    }
   ],
   "source": [
    "print('Similitud coseno:')\n",
    "print('Para el analista 0 en la serie', dicc_series[3], 'se tiene',calcula_opinion(opiniones, 0, cos_sim, 3))\n",
    "print('Similitud euclideana:')\n",
    "print('Para el analista 0 en la serie', dicc_series[3], 'se tiene',calcula_opinion(opiniones, 0, euclid_sim, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando SVD para reducir la dimensión\n",
    "\n",
    "Imaginemos el caso de una compañía como Netflix, cláramente, el número de usuarios rebasa al número de productos en su catálogo. En este tipo de situaciones, es conveniente reducir la dimensión de nuestros datos. Utilizando la descomposición SVD esto se realiza de la siguiente manera:\n",
    "\n",
    "+ Elegir un número $k$ de valores singulares (por ejemplo los $k$ más grades) y obtener la matriz diagonal $\\mathbf{\\Sigma_{k \\times k}}.$\n",
    "\n",
    "+ Definir la matriz $\\mathbf{U_{m \\times k}}$ con sólamente las primeras $k$ columnas de la matriz $\\mathbf{U_{m \\times m}}.$\n",
    "\n",
    "+ La matriz con la que ahora se trabajará es aquella que resulte del producto $\\left(\\mathbf{D_{m \\times n}^{T}}\\mathbf{U_{m \\times k}} \\mathbf{\\Sigma_{k \\times k}^{-1}}\\right)^{T}$ (matriz de dimensión $k \\times n$, $k < m$), esta matriz es igual a la submatriz formada por los primeros $k$ renglones de la matriz $\\mathbf{V^T}$.\n",
    "\n",
    "**Tarea: ¿Que matriz se utilizaría si tenemos más columnas que renglones?**.\n",
    "Teniendo los valores singulares, sabemos que la suma de estos pesos representa el 100%. Entonces se debe continuar sumando los pesos hasta que tenga el 90% de la información (rule of thumb). Si para obtener este porcentaje del 90% se tiene que reducir 4 columnas (opiniones), se deberá de hacer.\n",
    "\n",
    "```\n",
    "U, sig, Vt = np.linalg.svd(opiniones)\n",
    "sum(sig)\n",
    "```\n",
    "\n",
    "\n",
    "**Tarea moral: Intente conciliar lo que se establece en este notebook con las notas de la clase**\n",
    "Trabajaré en este...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_opinion_svd(opiniones, analista, fun_sim, serie_h, Vt, sig, umbral = 0.9):\n",
    "    '''\n",
    "    Función para calcular la opinión (promedio) que un analista tendría para una serie accionaria\n",
    "    \n",
    "    ENTRADA\n",
    "    opiniones: numpy.ndarray que representa las opiniones de un conjunto de analistas\n",
    "    \n",
    "    analista: Entero que representa el analista de interés (un índice de un renglón del arreglo opiniones)\n",
    "    \n",
    "    fun_sim: Función que se utilizará para calcular la similitud\n",
    "    \n",
    "    serie_h: Entero que representa la serie accionaria de interés (un índice de una columna del arreglo opiniones)\n",
    "    \n",
    "    Vt: Matriz V^{T} de la descomposición SVD\n",
    "    \n",
    "    sig: Arreglo con los valores singulares de la descomposición SVD\n",
    "\n",
    "    umbral: Número en (0,1) que representa la cantidad mínima de \"energía\"\n",
    "    explicada por los valores singulares (ver función selecciona_k_importantes)\n",
    "    \n",
    "    SALIDA\n",
    "    Opinión promedio de analista para la serie\n",
    "    '''\n",
    "    \n",
    "    #Si el analista ya dio una opinión para la serie, no hay nada que hacer\n",
    "    if opiniones[analista, serie_h] != 0:\n",
    "        #print('Para la serie ' + dicc_series[serie_h] + ' ya se tiene una opinión')\n",
    "        return opiniones[analista, serie_h]\n",
    "    \n",
    "    #número de series accionarias\n",
    "    n_series = opiniones.shape[1]\n",
    "    \n",
    "    #Para almacenar la similitud total y la similitud 'ponderada' por la opinión del analista\n",
    "    sim_total = 0.0; sim_pond = 0.0\n",
    "    \n",
    "    #Obtiene los k valores singulares más importantes\n",
    "    sig_k = selecciona_k_importantes(sig, umbral)\n",
    "    k = len(sig_k)\n",
    "    \n",
    "    #Submatriz de la matriz V^{T}\n",
    "    opiniones_svd = Vt[:k,:]\n",
    "    \n",
    "    #Para cada serie accionaria j\n",
    "    for j in range(0, n_series):\n",
    "        \n",
    "        #Opinión del analista para la serie j\n",
    "        opinion_serie = opiniones[analista, j]\n",
    "        \n",
    "        #Si el analista no tiene opinión para esta serie revisamos la siguiente\n",
    "        if opinion_serie == 0: continue\n",
    "        \n",
    "        similitud = fun_sim(opiniones_svd[:, serie_h], opiniones_svd[:, j])\n",
    "        \n",
    "        #se acumulan y se ponderan las similitudes\n",
    "        sim_total = sim_total + similitud\n",
    "        sim_pond = sim_pond + similitud * opinion_serie\n",
    "        \n",
    "    #Si no hay ninguna serie j para comparar la serie_h entonces no hay opinión\n",
    "    if sim_total == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        #Opinión promedio\n",
    "        return sim_pond / sim_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.92392178869397\n",
      "Similitud coseno:\n",
      "Para el analista 0 en la serie WALMEX se tiene 2.0341822781919645\n",
      "Similitud euclideana:\n",
      "Para el analista 0 en la serie WALMEX se tiene 2.0341822781919645\n"
     ]
    }
   ],
   "source": [
    "U, sig, Vt = np.linalg.svd(opiniones)\n",
    "umbral = 0.9\n",
    "print('Similitud coseno:')\n",
    "print('Para el analista 0 en la serie',\n",
    "      dicc_series[3], 'se tiene',calcula_opinion_svd(opiniones, 0, cos_sim, 3, Vt, sig, umbral ))\n",
    "print('Similitud euclideana:')\n",
    "print('Para el analista 0 en la serie', dicc_series[3],\n",
    "      'se tiene', calcula_opinion_svd(opiniones, 0, cos_sim, 3, Vt, sig, umbral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea\n",
    "\n",
    "+ Haga un programa para obtener la opinión de cada analista para cada serie accionaria en las cuales no ha emitido alguna opinión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_M():\n",
    "    '''\n",
    "    Función para obtener la Matriz de opiniones, la cual se busca descomponer.\n",
    "    Opiniones:\n",
    "    0 ⟹ todavía no se ha dado opinión.\n",
    "    1 ⟹ opinión negativa.\n",
    "    2 ⟹ opinión neutral.\n",
    "    3 ⟹ opinión positiva.\n",
    "    \n",
    "    ENTRADA\n",
    "    NaN\n",
    "    \n",
    "    SALIDA\n",
    "    M: Matriz de opiniones, la cual se busca descomponer.\n",
    "    '''\n",
    "   #Diccionario auxiliar\n",
    "    dicc_series = {\n",
    "        0:'FEMSA', \n",
    "        1:'AMX', \n",
    "        2:'GFNORTE', \n",
    "        3:'WALMEX', \n",
    "        4:'GMEXICO', \n",
    "        5:'CEMEX', \n",
    "        6:'BIMBO',\n",
    "        7:'GAP', \n",
    "        8:'ELEKTRA', \n",
    "        9:'TELEVISA'\n",
    "    }\n",
    "    #Se fija semilla con fines de reproducir el experimento\n",
    "    np.random.seed(54321)\n",
    "    #Se obtiene números aleatorios sobre la serie de opiniones\n",
    "    opiniones = np.random.randint(low = 0, high = 4, size = (100, 10))\n",
    "    #print(opiniones[0:10,]) #primeras diez opiniones\n",
    "    return opiniones\n",
    "    \n",
    "def reducing_opinions(M, U, sig, VT):\n",
    "    '''\n",
    "    Función que resolve el problema de los ceros en la matriz de opiniones mediante imputación. Imputación:\n",
    "    \n",
    "    * Se calcular la media de cada elemento (o usuario) y completar donde haya ceros en cada fila (o columna) de la matriz.\n",
    "    * Se normaliza cada fila, de modo que todos los elementos estén centrados alrededor de cero, por lo que los ceros se convertirán en el promedio.\n",
    "    \n",
    "    ENTRADA\n",
    "    M: Matriz de opiniones.\n",
    "    \n",
    "    SALIDA\n",
    "    M: Matriz de opiniones con imputación.\n",
    "    '''\n",
    "    \n",
    "    #Se itera la matriz\n",
    "    for i in range(0, np.shape(M)[0]):\n",
    "        for j in range(0, np.shape(M)[1]):\n",
    "            M[i, j] = calcula_opinion_svd(M, i, cos_sim, j, VT, sig)\n",
    "            \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 2 2 1 2 2 2 1 3]\n",
      " [3 2 2 2 1 3 2 3 2 3]\n",
      " [1 2 1 3 1 3 1 2 1 2]\n",
      " [2 2 3 2 2 3 2 3 2 2]\n",
      " [3 1 2 2 1 1 2 1 1 1]\n",
      " [3 3 3 2 1 3 2 2 3 1]\n",
      " [1 3 2 3 2 2 1 2 1 2]\n",
      " [2 1 3 3 2 3 2 1 1 1]\n",
      " [2 1 1 2 1 2 1 1 2 3]\n",
      " [1 1 1 2 1 1 1 1 1 1]\n",
      " [3 2 1 2 1 1 1 3 1 2]\n",
      " [2 2 1 1 1 1 1 1 2 1]\n",
      " [1 2 3 2 2 3 2 2 3 1]\n",
      " [2 3 2 3 2 2 2 3 2 1]\n",
      " [1 1 3 2 2 1 1 1 1 1]\n",
      " [2 2 3 3 3 2 1 1 2 1]\n",
      " [1 1 3 1 1 3 2 1 1 1]\n",
      " [2 2 3 2 2 1 1 3 1 2]\n",
      " [2 3 2 3 2 2 2 2 3 3]\n",
      " [1 2 1 2 1 1 1 2 2 2]\n",
      " [1 1 3 1 1 3 3 3 1 1]\n",
      " [1 3 2 3 2 1 1 1 1 2]\n",
      " [2 2 1 3 3 2 3 2 2 1]\n",
      " [2 1 1 3 2 1 1 1 2 2]\n",
      " [1 1 1 1 1 1 1 3 2 2]\n",
      " [1 2 1 1 1 2 1 3 1 1]\n",
      " [2 1 1 2 1 1 3 2 2 2]\n",
      " [1 3 3 1 1 1 2 2 3 1]\n",
      " [1 3 1 2 3 2 3 3 2 2]\n",
      " [2 1 3 2 3 3 1 1 1 1]\n",
      " [1 1 3 1 1 1 2 1 3 1]\n",
      " [3 1 1 2 2 2 3 2 2 3]\n",
      " [3 2 2 2 2 1 3 3 1 2]\n",
      " [3 2 1 2 2 1 1 2 2 3]\n",
      " [2 2 3 2 3 3 2 2 3 1]\n",
      " [1 1 2 1 1 2 1 3 2 3]\n",
      " [2 1 1 3 3 3 1 2 2 1]\n",
      " [3 3 2 2 3 2 2 2 1 1]\n",
      " [2 2 1 1 3 3 2 3 1 1]\n",
      " [3 3 2 3 3 2 2 2 2 2]\n",
      " [1 2 1 3 3 2 2 1 2 3]\n",
      " [1 1 1 1 2 1 1 2 2 2]\n",
      " [2 3 3 2 1 2 2 2 3 3]\n",
      " [3 2 1 2 3 3 3 2 1 1]\n",
      " [1 1 1 2 1 1 2 2 3 3]\n",
      " [2 1 3 3 2 3 1 3 2 2]\n",
      " [3 3 1 2 3 3 1 2 2 2]\n",
      " [2 2 2 3 2 3 3 2 2 3]\n",
      " [1 1 1 1 2 1 2 2 2 2]\n",
      " [2 3 2 2 3 1 2 2 2 2]\n",
      " [2 1 2 3 2 3 1 1 2 1]\n",
      " [2 2 2 3 3 1 1 3 2 3]\n",
      " [2 3 2 1 1 2 3 3 1 1]\n",
      " [1 2 3 3 2 2 3 3 2 3]\n",
      " [2 2 3 2 3 2 2 3 2 1]\n",
      " [3 3 3 1 2 1 2 2 2 3]\n",
      " [3 1 2 2 1 3 2 2 3 3]\n",
      " [2 2 2 1 1 1 2 2 2 1]\n",
      " [2 2 2 2 3 3 2 2 3 1]\n",
      " [1 2 1 1 1 1 1 1 3 1]\n",
      " [3 3 3 1 3 3 2 2 1 1]\n",
      " [3 2 2 2 3 3 2 2 1 1]\n",
      " [3 2 1 2 2 1 3 1 1 2]\n",
      " [3 1 2 2 3 3 3 2 3 2]\n",
      " [2 2 1 1 2 1 3 2 3 2]\n",
      " [3 2 1 2 3 2 2 2 3 2]\n",
      " [3 1 1 2 1 2 2 3 1 2]\n",
      " [1 2 3 3 3 2 2 3 1 1]\n",
      " [3 2 1 1 2 2 1 1 1 3]\n",
      " [1 1 2 2 1 1 1 1 2 2]\n",
      " [1 2 2 1 2 1 3 1 1 3]\n",
      " [1 1 1 3 1 1 2 2 3 2]\n",
      " [3 2 1 1 2 3 2 2 3 3]\n",
      " [1 3 2 1 1 1 1 2 1 1]\n",
      " [2 2 2 3 1 1 1 3 1 1]\n",
      " [1 3 1 3 1 1 2 1 1 2]\n",
      " [1 2 1 1 1 1 1 2 2 2]\n",
      " [1 1 2 1 2 2 1 3 1 3]\n",
      " [3 1 3 1 2 2 2 3 3 2]\n",
      " [1 1 1 1 1 3 2 3 1 1]\n",
      " [2 1 2 2 3 1 2 1 1 1]\n",
      " [2 1 1 1 3 1 1 1 2 3]\n",
      " [3 2 3 2 3 2 2 3 1 3]\n",
      " [3 2 2 1 1 1 1 2 1 1]\n",
      " [3 2 1 3 1 2 2 3 3 2]\n",
      " [2 3 2 2 2 3 2 3 1 2]\n",
      " [2 2 1 1 1 1 2 3 2 3]\n",
      " [2 2 3 2 3 2 2 3 1 1]\n",
      " [3 3 1 3 1 2 3 2 2 2]\n",
      " [3 2 1 2 2 1 3 3 2 3]\n",
      " [3 3 2 1 1 2 2 1 3 3]\n",
      " [3 1 1 3 2 2 1 1 2 1]\n",
      " [2 1 2 3 1 2 2 1 1 1]\n",
      " [1 3 2 1 1 1 1 1 2 2]\n",
      " [3 2 3 2 3 3 1 2 3 2]\n",
      " [1 1 1 3 1 2 2 1 1 3]\n",
      " [3 1 2 1 1 1 3 1 2 1]\n",
      " [1 2 1 2 2 2 2 3 2 1]\n",
      " [3 1 2 1 1 1 2 3 1 2]\n",
      " [1 3 3 1 1 1 2 1 1 1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Obteniendo matriz de opiniones, la cual se busca descomponer\n",
    "M = init_M()\n",
    "\n",
    "#Reduciendo matriz...\n",
    "\n",
    "#Obteniendo User feature matrix, Weights diagonal, Item feature matrix  \n",
    "U, sig, VT = np.linalg.svd(M)\n",
    "#Obteniendo Weights diagonal matrix\n",
    "m_sig = crea_matriz_sigma(sig, sig.shape[0], sig.shape[0])\n",
    "\n",
    "#Obteniendo nuevas opiniones\n",
    "M_hat = reducing_opinions(M, U, sig, VT)\n",
    "\n",
    "print(M_hat, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigación -  Referencias\n",
    "\n",
    "+ **Practical Recommender Systems**\n",
    "    + Author(s): ``Kim Falk``\n",
    "    + Publisher: Manning Publications\n",
    "    + Release Date: January 2019\n",
    "    + ISBN: 9781617292705\n",
    "\n",
    "+ **Recommender Systems Handbook**\n",
    "    + Author(s): ``Francesco Ricci`` - ``Lior Rokach``\n",
    "    + Springer books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the factorization using SVD\n",
    "\n",
    "One of the most commonly used methods for matrix factorization is an algorithm called $SVD$ (singular value decomposition). You want to find items to recommend to users, and you want to do it using extracted factors from the rating matrix. The idea of factorization is even more complicated because you want to end up with a formula that enables you to add new users and items without too much fuss.\n",
    "\n",
    "From the rating matrix $M$, you want to construct two matrices that you can use: one that represents the customer’s taste and one that contains the item profiles. Using SVD, you construct three matrices: $U$, $\\Sigma$, and $V^T$ (also known as $V*$ depending on which book you look in). Because you want to end up with two matrices, you multiply the square root of $\\Sigma$ on one of the two others, and then you only have two left. But before doing that, you want to use the middle matrix that gives you information about how much you should reduce the dimensions.\n",
    "\n",
    "<br>\n",
    "<center><i>Figure 11.7. A matrix can be factorized into three matrices.</i></center>\n",
    "\n",
    "<img src=\"https://learning.oreilly.com/library/view/practical-recommender-systems/9781617292705/11fig07_alt.jpg\" alt=\"Figure 11.7. A matrix can be factorized into three matrices.\">\n",
    "\n",
    "You call them:\n",
    "\n",
    "1. $M$ — A matrix you want to decompose; in your case, it’s the rating matrix.\n",
    "1. $U$ — User feature matrix.\n",
    "1. $Σ$ — Weights diagonal.\n",
    "1. $V^T$ — Item feature matrix.\n",
    "\n",
    "When using the $SVD$ algorithm the $\\Sigma$ will always be a diagonal matrix.\n",
    "\n",
    "**Diagonal matrix**\n",
    "\n",
    "A diagonal matrix means that it only has zeros except in the diagonal from the upper-left corner to the lower-right corner as shown here:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    9 & 0 & 0 \\\\[0.3em]\n",
    "    0 & 5 & 0 \\\\[0.3em]\n",
    "    0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "**Reducing the matrix**\n",
    "\n",
    "It might be hard to see how splitting a matrix into three matrices does any good, especially when I mentioned that it’s time-consuming to create these. But the idea is that the central diagonal matrix $\\Sigma$ contains elements that are sorted from the largest to the smallest; the elements are called $singular values$, and the values indicate how much information this feature produces for the data set. A $feature$ here means both a column in the user matrix $U$ and a row in the content matrix $V^T$. You can now select a number $r$ of features and set the rest of the diagonal to zero. Look at figure 11.8, which illustrates what remains of the matrices when you set diagonal values to zero outside the middle box. This is the same as removing all right-most columns in the user matrix $U$ and all the bottom rows from $V*$, keeping only the $r$ top rows.\n",
    "\n",
    "<br>\n",
    "<center><i>Figure 11.8. Reducing the SVD by setting small values in Σ to zero.</i></center>\n",
    "\n",
    "<img src=\"https://learning.oreilly.com/library/view/practical-recommender-systems/9781617292705/11fig08_alt.jpg\" alt=\"Figure 11.8. Reducing the SVD by setting small values in Σ to zero.\">\n",
    "\n",
    "<br><br>\n",
    "<center><i>Figure 11.9. The matrices of the NumPy factorization.</i></center>\n",
    "\n",
    "<img src=\"https://learning.oreilly.com/library/view/practical-recommender-systems/9781617292705/11fig09_alt.jpg\" alt=\"Figure 11.9. The matrices of the NumPy factorization.\">\n",
    "\n",
    "Does that make sense? Well, probably, not really. It’s so much work to get three matrices exactly the same size as the original. But hold your horses a second. Look at the diagonal matrix in the middle ($\\Sigma$), more significantly known as the $weights matrix$. Each of these weights can be an indication of how much information is present in each dimension. The first one provides much information (17.27), the next one, not so much (5.84), and so on, so you can reduce the size of the matrices. But by how much?\n",
    "\n",
    "**How much should the matrix be reduced?**\n",
    "\n",
    "You could reduce the dimensions using only two, and still produce a chart like the one in figure 11.2. Another good reason for reducing the matrix to two dimensions is that by looking at the weights in the Sigma matrix ($\\Sigma$), you’ll get most of the information by using only two features.\n",
    "\n",
    "With such a small example, reducing the matrix doesn’t matter much. But a rule of thumb is that you should retain 90% of the information. If you add up all the weights, that’s 100%, then you should continue counting weights until you have 90% of the information. Let’s do the calculation for the matrix in figure 11.9.\n",
    "\n",
    "The sum of all of them is 32.0, so 90% of that is 28.83. If you reduce it to 4 dimensions, then the weight is 29.80 so you should reduce it to 4 dimensions (factors). To reduce the matrices in code, you’d implement the commands in this listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "The original and most simple implementation of this approach makes recommendations to the active user based on items that other users with similar tastes liked in the past. The similarity in taste of two users is calculated based on the similarity in the rating history of the users. This is the reason why refers to collaborative filtering as “people-to-people correlation”. Collaborative filtering is considered to be the most popular and widely implemented technique in RS.\n",
    "\n",
    "Specifically, the authors discuss latent factor models, such as matrix factorization (e.g., Singular Value Decomposition (SVD)). These methods transform both items and users to the same latent factor space. The latent space is then used to explain ratings by characterizing both products and users in term of factors automatically inferred from user feedback. The authors elucidate how SVD can handle additional features of the data, including implicit feedback and temporal information. They also describe techniques to address shortcomings of neighborhood techniques by suggesting more rigorous formulations using global optimization techniques. Utilizing such techniques makes it possible to lift the limit on neighborhood size and to address implicit feedback and temporal dynamics. The resulting accuracy is close to that of matrix factorization models, while offering a number of practical advantages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
