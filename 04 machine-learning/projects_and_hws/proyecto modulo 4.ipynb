{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from ipywidgets import interact, FloatSlider\n",
    "from numpy.random import seed, randn\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer #para quitar NAN con promedios\n",
    "from sklearn.preprocessing import OneHotEncoder #cuando tengo informacion cualitativa la transfomo en valores 0 y 1\n",
    "from sklearn.compose import ColumnTransformer  #me deja empaquetar trnasformaciones\n",
    "from sklearn.compose import make_column_selector  # me permite seleccionar columnas no por nombre, si no por caracteritcias\n",
    "#ejemplo: que sean sus valores \"float\"\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.special import gamma\n",
    "from scipy.optimize import minimize\n",
    "from numpy.random import seed\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_validate, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\"\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "plt.rcParams[\"font.size\"] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESUMEN:\n",
    "#EL ORDEN DEBE DE SER:\n",
    "#1) SELECCIONAR MI X Y Y TRAIN Y TEST\n",
    "#2) CREAR MI CONJUNTO DE TRANFOMRACIONES A MI INFORMACION CON UN COLUMN TRANSFORMER\n",
    "#3) CREAR UN PIPE QUE CONTENGA LAS COLUMNAS VIA EL COLUMN TRANSFORMER MAS EL MODELO QUE QUIERO APLICARLE\n",
    "#4) APLICAR UN CROSS VALIDATE O UN GRIDSEARCH SOBRE EL PIPE DE 2)\n",
    "#5) SI EN 4) APLICASTE GRIDSEARCH Y CONSIGUES UN BEST ESTIMATOR (MEJOR MODELO), APLICO EL BEST ESTIMATOR SOBRE UN CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\"./final-project-predict-home-value/files/house-prices-advanced-regression-techniques/train.csv\")  #importo mi informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>14115</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10084</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10382</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>350</td>\n",
       "      <td>11</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>190</td>\n",
       "      <td>RL</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7420</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11200</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11924</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12968</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR2</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>91.0</td>\n",
       "      <td>10652</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10920</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdWo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>45</td>\n",
       "      <td>RM</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11241</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>90</td>\n",
       "      <td>RL</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10791</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>90000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>13695</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7560</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>COD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>101.0</td>\n",
       "      <td>14215</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>RM</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7449</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Grvl</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Bnk</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9742</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>120</td>\n",
       "      <td>RM</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4224</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8246</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>110.0</td>\n",
       "      <td>14230</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>7200</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>98.0</td>\n",
       "      <td>11478</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>47.0</td>\n",
       "      <td>16321</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>RM</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6324</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>68500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>1431</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>21930</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR3</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>192140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>1432</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4928</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>143750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>1433</td>\n",
       "      <td>30</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10800</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Grvl</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>64500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>1434</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>93.0</td>\n",
       "      <td>10261</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>1435</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>17400</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Low</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>1436</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8400</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "      <td>COD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>1437</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdWo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1438</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>96.0</td>\n",
       "      <td>12444</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2008</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>394617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>1439</td>\n",
       "      <td>20</td>\n",
       "      <td>RM</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7407</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>1440</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11584</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>1441</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>79.0</td>\n",
       "      <td>11526</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Bnk</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>1442</td>\n",
       "      <td>120</td>\n",
       "      <td>RM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4426</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>149300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>1443</td>\n",
       "      <td>60</td>\n",
       "      <td>FV</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11003</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>1444</td>\n",
       "      <td>30</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8854</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>1445</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>63.0</td>\n",
       "      <td>8500</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>1446</td>\n",
       "      <td>85</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8400</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>1447</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26142</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>1448</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>1449</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11767</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdWo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>1450</td>\n",
       "      <td>180</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1533</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>92000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>1451</td>\n",
       "      <td>90</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>1452</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9262</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>287090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>1453</td>\n",
       "      <td>180</td>\n",
       "      <td>RM</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3675</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>1454</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>90.0</td>\n",
       "      <td>17217</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>84500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>1455</td>\n",
       "      <td>20</td>\n",
       "      <td>FV</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7500</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "5        6          50       RL         85.0    14115   Pave   NaN      IR1   \n",
       "6        7          20       RL         75.0    10084   Pave   NaN      Reg   \n",
       "7        8          60       RL          NaN    10382   Pave   NaN      IR1   \n",
       "8        9          50       RM         51.0     6120   Pave   NaN      Reg   \n",
       "9       10         190       RL         50.0     7420   Pave   NaN      Reg   \n",
       "10      11          20       RL         70.0    11200   Pave   NaN      Reg   \n",
       "11      12          60       RL         85.0    11924   Pave   NaN      IR1   \n",
       "12      13          20       RL          NaN    12968   Pave   NaN      IR2   \n",
       "13      14          20       RL         91.0    10652   Pave   NaN      IR1   \n",
       "14      15          20       RL          NaN    10920   Pave   NaN      IR1   \n",
       "15      16          45       RM         51.0     6120   Pave   NaN      Reg   \n",
       "16      17          20       RL          NaN    11241   Pave   NaN      IR1   \n",
       "17      18          90       RL         72.0    10791   Pave   NaN      Reg   \n",
       "18      19          20       RL         66.0    13695   Pave   NaN      Reg   \n",
       "19      20          20       RL         70.0     7560   Pave   NaN      Reg   \n",
       "20      21          60       RL        101.0    14215   Pave   NaN      IR1   \n",
       "21      22          45       RM         57.0     7449   Pave  Grvl      Reg   \n",
       "22      23          20       RL         75.0     9742   Pave   NaN      Reg   \n",
       "23      24         120       RM         44.0     4224   Pave   NaN      Reg   \n",
       "24      25          20       RL          NaN     8246   Pave   NaN      IR1   \n",
       "25      26          20       RL        110.0    14230   Pave   NaN      Reg   \n",
       "26      27          20       RL         60.0     7200   Pave   NaN      Reg   \n",
       "27      28          20       RL         98.0    11478   Pave   NaN      Reg   \n",
       "28      29          20       RL         47.0    16321   Pave   NaN      IR1   \n",
       "29      30          30       RM         60.0     6324   Pave   NaN      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "1430  1431          60       RL         60.0    21930   Pave   NaN      IR3   \n",
       "1431  1432         120       RL          NaN     4928   Pave   NaN      IR1   \n",
       "1432  1433          30       RL         60.0    10800   Pave  Grvl      Reg   \n",
       "1433  1434          60       RL         93.0    10261   Pave   NaN      IR1   \n",
       "1434  1435          20       RL         80.0    17400   Pave   NaN      Reg   \n",
       "1435  1436          20       RL         80.0     8400   Pave   NaN      Reg   \n",
       "1436  1437          20       RL         60.0     9000   Pave   NaN      Reg   \n",
       "1437  1438          20       RL         96.0    12444   Pave   NaN      Reg   \n",
       "1438  1439          20       RM         90.0     7407   Pave   NaN      Reg   \n",
       "1439  1440          60       RL         80.0    11584   Pave   NaN      Reg   \n",
       "1440  1441          70       RL         79.0    11526   Pave   NaN      IR1   \n",
       "1441  1442         120       RM          NaN     4426   Pave   NaN      Reg   \n",
       "1442  1443          60       FV         85.0    11003   Pave   NaN      Reg   \n",
       "1443  1444          30       RL          NaN     8854   Pave   NaN      Reg   \n",
       "1444  1445          20       RL         63.0     8500   Pave   NaN      Reg   \n",
       "1445  1446          85       RL         70.0     8400   Pave   NaN      Reg   \n",
       "1446  1447          20       RL          NaN    26142   Pave   NaN      IR1   \n",
       "1447  1448          60       RL         80.0    10000   Pave   NaN      Reg   \n",
       "1448  1449          50       RL         70.0    11767   Pave   NaN      Reg   \n",
       "1449  1450         180       RM         21.0     1533   Pave   NaN      Reg   \n",
       "1450  1451          90       RL         60.0     9000   Pave   NaN      Reg   \n",
       "1451  1452          20       RL         78.0     9262   Pave   NaN      Reg   \n",
       "1452  1453         180       RM         35.0     3675   Pave   NaN      Reg   \n",
       "1453  1454          20       RL         90.0    17217   Pave   NaN      Reg   \n",
       "1454  1455          20       FV         62.0     7500   Pave  Pave      Reg   \n",
       "1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
       "1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
       "1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
       "1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
       "1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "5            Lvl    AllPub  ...        0    NaN  MnPrv        Shed     700   \n",
       "6            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "7            Lvl    AllPub  ...        0    NaN    NaN        Shed     350   \n",
       "8            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "9            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "10           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "11           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "12           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "13           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "14           Lvl    AllPub  ...        0    NaN   GdWo         NaN       0   \n",
       "15           Lvl    AllPub  ...        0    NaN  GdPrv         NaN       0   \n",
       "16           Lvl    AllPub  ...        0    NaN    NaN        Shed     700   \n",
       "17           Lvl    AllPub  ...        0    NaN    NaN        Shed     500   \n",
       "18           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "19           Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "20           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "21           Bnk    AllPub  ...        0    NaN  GdPrv         NaN       0   \n",
       "22           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "23           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "24           Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "25           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "26           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "27           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "28           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "29           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
       "1430         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1431         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1432         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1433         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1434         Low    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1435         Lvl    AllPub  ...        0    NaN  GdPrv         NaN       0   \n",
       "1436         Lvl    AllPub  ...        0    NaN   GdWo         NaN       0   \n",
       "1437         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1438         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "1439         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1440         Bnk    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1441         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1442         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1443         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1444         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1445         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1446         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1447         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1448         Lvl    AllPub  ...        0    NaN   GdWo         NaN       0   \n",
       "1449         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1450         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1451         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1452         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1453         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1454         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n",
       "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "\n",
       "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0         2   2008        WD         Normal     208500  \n",
       "1         5   2007        WD         Normal     181500  \n",
       "2         9   2008        WD         Normal     223500  \n",
       "3         2   2006        WD        Abnorml     140000  \n",
       "4        12   2008        WD         Normal     250000  \n",
       "5        10   2009        WD         Normal     143000  \n",
       "6         8   2007        WD         Normal     307000  \n",
       "7        11   2009        WD         Normal     200000  \n",
       "8         4   2008        WD        Abnorml     129900  \n",
       "9         1   2008        WD         Normal     118000  \n",
       "10        2   2008        WD         Normal     129500  \n",
       "11        7   2006       New        Partial     345000  \n",
       "12        9   2008        WD         Normal     144000  \n",
       "13        8   2007       New        Partial     279500  \n",
       "14        5   2008        WD         Normal     157000  \n",
       "15        7   2007        WD         Normal     132000  \n",
       "16        3   2010        WD         Normal     149000  \n",
       "17       10   2006        WD         Normal      90000  \n",
       "18        6   2008        WD         Normal     159000  \n",
       "19        5   2009       COD        Abnorml     139000  \n",
       "20       11   2006       New        Partial     325300  \n",
       "21        6   2007        WD         Normal     139400  \n",
       "22        9   2008        WD         Normal     230000  \n",
       "23        6   2007        WD         Normal     129900  \n",
       "24        5   2010        WD         Normal     154000  \n",
       "25        7   2009        WD         Normal     256300  \n",
       "26        5   2010        WD         Normal     134800  \n",
       "27        5   2010        WD         Normal     306000  \n",
       "28       12   2006        WD         Normal     207500  \n",
       "29        5   2008        WD         Normal      68500  \n",
       "...     ...    ...       ...            ...        ...  \n",
       "1430      7   2006        WD         Normal     192140  \n",
       "1431     10   2009        WD         Normal     143750  \n",
       "1432      8   2007        WD         Normal      64500  \n",
       "1433      5   2008        WD         Normal     186500  \n",
       "1434      5   2006        WD         Normal     160000  \n",
       "1435      7   2008       COD        Abnorml     174000  \n",
       "1436      5   2007        WD         Normal     120500  \n",
       "1437     11   2008       New        Partial     394617  \n",
       "1438      4   2010        WD         Normal     149700  \n",
       "1439     11   2007        WD         Normal     197000  \n",
       "1440      9   2008        WD         Normal     191000  \n",
       "1441      5   2008        WD         Normal     149300  \n",
       "1442      4   2009        WD         Normal     310000  \n",
       "1443      5   2009        WD         Normal     121000  \n",
       "1444     11   2007        WD         Normal     179600  \n",
       "1445      5   2007        WD         Normal     129000  \n",
       "1446      4   2010        WD         Normal     157900  \n",
       "1447     12   2007        WD         Normal     240000  \n",
       "1448      5   2007        WD         Normal     112000  \n",
       "1449      8   2006        WD        Abnorml      92000  \n",
       "1450      9   2009        WD         Normal     136000  \n",
       "1451      5   2009       New        Partial     287090  \n",
       "1452      5   2006        WD         Normal     145000  \n",
       "1453      7   2006        WD        Abnorml      84500  \n",
       "1454     10   2009        WD         Normal     185000  \n",
       "1455      8   2007        WD         Normal     175000  \n",
       "1456      2   2010        WD         Normal     210000  \n",
       "1457      5   2010        WD         Normal     266500  \n",
       "1458      4   2010        WD         Normal     142125  \n",
       "1459      6   2008        WD         Normal     147500  \n",
       "\n",
       "[1460 rows x 81 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1452.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.685262</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>...</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.610009</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>181.066207</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>...</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>365.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1095.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
       "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
       "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
       "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
       "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
       "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
       "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
       "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \n",
       "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \n",
       "std       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n",
       "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n",
       "\n",
       "        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\n",
       "count  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     94.244521    46.660274      21.954110     3.409589    15.060959   \n",
       "std     125.338794    66.256028      61.119149    29.317331    55.757415   \n",
       "min       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n",
       "75%     168.000000    68.000000       0.000000     0.000000     0.000000   \n",
       "max     857.000000   547.000000     552.000000   508.000000   480.000000   \n",
       "\n",
       "          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \n",
       "count  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \n",
       "mean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \n",
       "std      40.177307    496.123024     2.703626     1.328095   79442.502883  \n",
       "min       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n",
       "25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n",
       "50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n",
       "75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \n",
       "max     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      1460.000000\n",
       "mean     180921.195890\n",
       "std       79442.502883\n",
       "min       34900.000000\n",
       "25%      129975.000000\n",
       "50%      163000.000000\n",
       "75%      214000.000000\n",
       "max      755000.000000\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.SalePrice.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1ffb98d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 324,
       "width": 606
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(housing.SalePrice, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1)\n",
    "#Determino mi X y y, asi como los train y test sets\n",
    "train, test = train_test_split(housing, test_size=0.2, random_state=2718) #primero divido toda la info en train y test\n",
    "xtrain, ytrain = train.drop(\"SalePrice\", axis=1), train[\"SalePrice\"] #determino quien es X y quien y train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>761</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>9100</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>450</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>963</td>\n",
       "      <td>160</td>\n",
       "      <td>RL</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2308</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>955</td>\n",
       "      <td>90</td>\n",
       "      <td>RL</td>\n",
       "      <td>35.0</td>\n",
       "      <td>9400</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>AdjLand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>1287</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9790</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>1025</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15498</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>COD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>1331</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>812</td>\n",
       "      <td>120</td>\n",
       "      <td>RM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4438</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>ConLD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>290</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8730</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>259</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>444</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>53.0</td>\n",
       "      <td>3922</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>690</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>61.0</td>\n",
       "      <td>7577</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>311</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7685</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>1023</td>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>52.0</td>\n",
       "      <td>9439</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11616</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>869</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14762</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR2</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>903</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>63.0</td>\n",
       "      <td>7875</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>245</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8880</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>740</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>9313</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>1185</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>50.0</td>\n",
       "      <td>35133</td>\n",
       "      <td>Grvl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>870</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9938</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>694</td>\n",
       "      <td>30</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5400</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>579</td>\n",
       "      <td>160</td>\n",
       "      <td>FV</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3604</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>160</td>\n",
       "      <td>RL</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2280</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>250</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR2</td>\n",
       "      <td>Low</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>500</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>596</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>69.0</td>\n",
       "      <td>11302</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>1026</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7700</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>180</td>\n",
       "      <td>30</td>\n",
       "      <td>RM</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8520</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>1336</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9650</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>1137</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>1320</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10215</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Bnk</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8070</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>151</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>120.0</td>\n",
       "      <td>10356</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>577</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6292</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>262</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>69.0</td>\n",
       "      <td>9588</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2007</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>617</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7861</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>94</td>\n",
       "      <td>190</td>\n",
       "      <td>C (all)</td>\n",
       "      <td>60.0</td>\n",
       "      <td>7200</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>495</td>\n",
       "      <td>30</td>\n",
       "      <td>RM</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5784</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>1244</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>107.0</td>\n",
       "      <td>13891</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>1203</td>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>772</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>67.0</td>\n",
       "      <td>8877</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>COD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>214</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>13568</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR2</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>670</td>\n",
       "      <td>30</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>356</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>105.0</td>\n",
       "      <td>11249</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR2</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>1136</td>\n",
       "      <td>30</td>\n",
       "      <td>RM</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6180</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>1233</td>\n",
       "      <td>90</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>9842</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>170</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16669</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>172</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>141.0</td>\n",
       "      <td>31770</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>1326</td>\n",
       "      <td>30</td>\n",
       "      <td>RM</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3636</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>575</td>\n",
       "      <td>80</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>10500</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdWo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>1044</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>86.0</td>\n",
       "      <td>11839</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>614</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8402</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2007</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>357</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9248</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>828</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8529</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>785</td>\n",
       "      <td>75</td>\n",
       "      <td>RM</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6300</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Grvl</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>1124</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9405</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>147</td>\n",
       "      <td>30</td>\n",
       "      <td>RM</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>751</td>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>55.0</td>\n",
       "      <td>8800</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Grvl</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>390</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>96.0</td>\n",
       "      <td>12474</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2008</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>149</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>63.0</td>\n",
       "      <td>7500</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>1050</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>11100</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Low</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>1397</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57200</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Bnk</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1168 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "760    761          20       RL         70.0     9100   Pave   NaN      Reg   \n",
       "962    963         160       RL         24.0     2308   Pave   NaN      Reg   \n",
       "954    955          90       RL         35.0     9400   Pave   NaN      IR1   \n",
       "1286  1287          20       RL          NaN     9790   Pave   NaN      Reg   \n",
       "1024  1025          20       RL          NaN    15498   Pave   NaN      IR1   \n",
       "1330  1331          20       RL         85.0    10000   Pave   NaN      Reg   \n",
       "811    812         120       RM          NaN     4438   Pave   NaN      Reg   \n",
       "289    290          70       RL         60.0     8730   Pave   NaN      Reg   \n",
       "443    444         120       RL         53.0     3922   Pave   NaN      Reg   \n",
       "689    690         120       RL         61.0     7577   Pave   NaN      IR1   \n",
       "310    311          60       RL          NaN     7685   Pave   NaN      IR1   \n",
       "1022  1023          50       RM         52.0     9439   Pave   NaN      Reg   \n",
       "116    117          20       RL          NaN    11616   Pave   NaN      Reg   \n",
       "868    869          60       RL          NaN    14762   Pave   NaN      IR2   \n",
       "902    903          60       RL         63.0     7875   Pave   NaN      Reg   \n",
       "244    245          60       RL          NaN     8880   Pave   NaN      IR1   \n",
       "739    740          60       RL         65.0     9313   Pave   NaN      IR1   \n",
       "1184  1185          20       RL         50.0    35133   Grvl   NaN      Reg   \n",
       "869    870          60       RL         80.0     9938   Pave   NaN      Reg   \n",
       "693    694          30       RL         60.0     5400   Pave   NaN      Reg   \n",
       "578    579         160       FV         34.0     3604   Pave  Pave      Reg   \n",
       "195    196         160       RL         24.0     2280   Pave   NaN      Reg   \n",
       "249    250          50       RL          NaN   159000   Pave   NaN      IR2   \n",
       "595    596          20       RL         69.0    11302   Pave   NaN      IR1   \n",
       "1025  1026          20       RL         70.0     7700   Pave   NaN      Reg   \n",
       "179    180          30       RM         60.0     8520   Pave   NaN      Reg   \n",
       "1335  1336          20       RL         80.0     9650   Pave   NaN      Reg   \n",
       "1136  1137          50       RL         80.0     9600   Pave   NaN      Reg   \n",
       "1319  1320          20       RL         75.0    10215   Pave   NaN      Reg   \n",
       "89      90          20       RL         60.0     8070   Pave   NaN      Reg   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "150    151          20       RL        120.0    10356   Pave   NaN      Reg   \n",
       "576    577          50       RL         52.0     6292   Pave   NaN      Reg   \n",
       "261    262          60       RL         69.0     9588   Pave   NaN      IR1   \n",
       "616    617          60       RL          NaN     7861   Pave   NaN      IR1   \n",
       "93      94         190  C (all)         60.0     7200   Pave   NaN      Reg   \n",
       "494    495          30       RM         50.0     5784   Pave   NaN      Reg   \n",
       "1243  1244          20       RL        107.0    13891   Pave   NaN      Reg   \n",
       "1202  1203          50       RM         50.0     6000   Pave   NaN      Reg   \n",
       "771    772          20       RL         67.0     8877   Pave   NaN      Reg   \n",
       "213    214          20       RL         43.0    13568   Pave   NaN      IR2   \n",
       "669    670          30       RL         80.0    11600   Pave   NaN      Reg   \n",
       "355    356          20       RL        105.0    11249   Pave   NaN      IR2   \n",
       "1135  1136          30       RM         60.0     6180   Pave   NaN      Reg   \n",
       "1232  1233          90       RL         70.0     9842   Pave   NaN      Reg   \n",
       "169    170          20       RL          NaN    16669   Pave   NaN      IR1   \n",
       "171    172          20       RL        141.0    31770   Pave   NaN      IR1   \n",
       "1325  1326          30       RM         40.0     3636   Pave   NaN      Reg   \n",
       "574    575          80       RL         70.0    10500   Pave   NaN      Reg   \n",
       "1043  1044          60       RL         86.0    11839   Pave   NaN      Reg   \n",
       "613    614          20       RL         70.0     8402   Pave   NaN      Reg   \n",
       "356    357          20       RL          NaN     9248   Pave   NaN      IR1   \n",
       "827    828          20       RL         65.0     8529   Pave   NaN      IR1   \n",
       "784    785          75       RM         35.0     6300   Pave  Grvl      Reg   \n",
       "1123  1124          20       RL         50.0     9405   Pave   NaN      Reg   \n",
       "146    147          30       RM         51.0     6120   Pave   NaN      Reg   \n",
       "750    751          50       RM         55.0     8800   Pave  Grvl      Reg   \n",
       "389    390          60       RL         96.0    12474   Pave   NaN      Reg   \n",
       "148    149          20       RL         63.0     7500   Pave   NaN      Reg   \n",
       "1049  1050          20       RL         60.0    11100   Pave   NaN      Reg   \n",
       "1396  1397          20       RL          NaN    57200   Pave   NaN      IR1   \n",
       "\n",
       "     LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence  \\\n",
       "760          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "962          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "954          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1286         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1024         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1330         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "811          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "289          Lvl    AllPub  ...         259        0    NaN    NaN   \n",
       "443          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "689          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "310          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1022         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "116          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "868          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "902          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "244          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "739          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1184         Lvl    AllPub  ...         263        0    NaN    NaN   \n",
       "869          Lvl    AllPub  ...           0        0    NaN  GdPrv   \n",
       "693          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "578          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "195          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "249          Low    AllPub  ...           0        0    NaN    NaN   \n",
       "595          Lvl    AllPub  ...         120        0    NaN    NaN   \n",
       "1025         Lvl    AllPub  ...           0        0    NaN  MnPrv   \n",
       "179          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1335         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1136         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1319         Bnk    AllPub  ...           0        0    NaN    NaN   \n",
       "89           Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "...          ...       ...  ...         ...      ...    ...    ...   \n",
       "150          Lvl    AllPub  ...           0        0    NaN  MnPrv   \n",
       "576          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "261          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "616          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "93           Lvl    AllPub  ...          99        0    NaN    NaN   \n",
       "494          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1243         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1202         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "771          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "213          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "669          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "355          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1135         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1232         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "169          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "171          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1325         Lvl    AllPub  ...           0        0    NaN  MnPrv   \n",
       "574          Lvl    AllPub  ...           0        0    NaN   GdWo   \n",
       "1043         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "613          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "356          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "827          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "784          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1123         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "146          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "750          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "389          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "148          Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1049         Low    AllPub  ...           0        0    NaN    NaN   \n",
       "1396         Bnk    AllPub  ...           0        0    NaN    NaN   \n",
       "\n",
       "     MiscFeature MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "760         Shed     450     10    2009        WD         Normal  \n",
       "962          NaN       0      7    2007        WD         Normal  \n",
       "954          NaN       0     10    2006        WD        AdjLand  \n",
       "1286         NaN       0      6    2010        WD         Normal  \n",
       "1024         NaN       0      5    2008       COD        Abnorml  \n",
       "1330         NaN       0     12    2007        WD         Normal  \n",
       "811          NaN       0      6    2008     ConLD         Normal  \n",
       "289          NaN       0      7    2007        WD         Normal  \n",
       "443          NaN       0      6    2007       New        Partial  \n",
       "689          NaN       0      6    2007        WD         Normal  \n",
       "310          NaN       0      5    2006        WD         Normal  \n",
       "1022         NaN       0      3    2007        WD         Normal  \n",
       "116          NaN       0      9    2009        WD         Normal  \n",
       "868          NaN       0      5    2006        WD         Normal  \n",
       "902          NaN       0      7    2006        WD         Normal  \n",
       "244          NaN       0      5    2010        WD         Normal  \n",
       "739          NaN       0      4    2009        WD         Normal  \n",
       "1184         NaN       0      5    2007        WD         Normal  \n",
       "869          NaN       0      6    2010        WD         Normal  \n",
       "693          NaN       0     12    2006        WD        Abnorml  \n",
       "578          NaN       0      2    2008        WD        Abnorml  \n",
       "195          NaN       0      7    2009        WD         Normal  \n",
       "249         Shed     500      6    2007        WD         Normal  \n",
       "595          NaN       0      8    2006       New        Partial  \n",
       "1025         NaN       0      3    2007        WD         Normal  \n",
       "179          NaN       0      7    2007        WD         Normal  \n",
       "1335         NaN       0      4    2009        WD         Normal  \n",
       "1136         NaN       0      4    2008        WD        Abnorml  \n",
       "1319         NaN       0      2    2007        WD         Normal  \n",
       "89           NaN       0      8    2007        WD         Normal  \n",
       "...          ...     ...    ...     ...       ...            ...  \n",
       "150          NaN       0      1    2007        WD         Normal  \n",
       "576          NaN       0      8    2009        WD         Normal  \n",
       "261          NaN       0     11    2007       New        Partial  \n",
       "616          NaN       0      6    2006        WD         Normal  \n",
       "93           NaN       0     11    2007        WD         Normal  \n",
       "494          NaN       0     12    2009        WD         Normal  \n",
       "1243         NaN       0      9    2006       New        Partial  \n",
       "1202         NaN       0      5    2009        WD         Normal  \n",
       "771          NaN       0      4    2006       COD         Normal  \n",
       "213          NaN       0      7    2006        WD         Normal  \n",
       "669          NaN       0      7    2006        WD         Normal  \n",
       "355          NaN       0      8    2007        WD         Normal  \n",
       "1135         NaN       0      5    2007        WD         Normal  \n",
       "1232         NaN       0      3    2007        WD         Normal  \n",
       "169          NaN       0      1    2006        WD         Normal  \n",
       "171          NaN       0      5    2010        WD         Normal  \n",
       "1325         NaN       0      1    2008        WD         Normal  \n",
       "574          NaN       0     12    2007        WD         Normal  \n",
       "1043         NaN       0      5    2008        WD         Normal  \n",
       "613          NaN       0     12    2007       New        Partial  \n",
       "356          NaN       0      7    2009        WD         Normal  \n",
       "827          NaN       0      4    2009        WD         Normal  \n",
       "784          NaN       0      6    2008        WD         Normal  \n",
       "1123         NaN       0      6    2009        WD         Normal  \n",
       "146          NaN       0     11    2009        WD         Normal  \n",
       "750          NaN       0      6    2010        WD         Normal  \n",
       "389          NaN       0      8    2008       New        Partial  \n",
       "148          NaN       0      4    2008        WD         Normal  \n",
       "1049         NaN       0      4    2010        WD        Abnorml  \n",
       "1396         NaN       0      6    2010        WD         Normal  \n",
       "\n",
       "[1168 rows x 80 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760     127500\n",
       "962     155000\n",
       "954     127500\n",
       "1286    143000\n",
       "1024    287000\n",
       "1330    227000\n",
       "811     144500\n",
       "289     153575\n",
       "443     172500\n",
       "689     194700\n",
       "310     165600\n",
       "1022     87000\n",
       "116     139000\n",
       "868     169000\n",
       "902     180000\n",
       "244     205000\n",
       "739     190000\n",
       "1184    186700\n",
       "869     236000\n",
       "693     108480\n",
       "578     146000\n",
       "195     148500\n",
       "249     277000\n",
       "595     319000\n",
       "1025    112500\n",
       "179     100000\n",
       "1335    167900\n",
       "1136    119000\n",
       "1319    111000\n",
       "89      123600\n",
       "         ...  \n",
       "150     122000\n",
       "576     145000\n",
       "261     276000\n",
       "616     183200\n",
       "93      133900\n",
       "494      91300\n",
       "1243    465000\n",
       "1202    117000\n",
       "771     102000\n",
       "213     156000\n",
       "669     137500\n",
       "355     177500\n",
       "1135    102000\n",
       "1232    101800\n",
       "169     228000\n",
       "171     215000\n",
       "1325     55000\n",
       "574     139000\n",
       "1043    262280\n",
       "613     147000\n",
       "356     173000\n",
       "827     189000\n",
       "784     128000\n",
       "1123    118000\n",
       "146     105000\n",
       "750      96500\n",
       "389     426000\n",
       "148     141000\n",
       "1049     84900\n",
       "1396    160000\n",
       "Name: SalePrice, Length: 1168, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean r2: 0.79\n",
      "mean mae: 22,235.54\n"
     ]
    }
   ],
   "source": [
    "#Aplicando un modelo Linear Regression\n",
    "#1)\n",
    "#Determino mi X y y, asi como los train y test sets\n",
    "train, test = train_test_split(housing, test_size=0.2, random_state=2718) #primero divido toda la info en train y test\n",
    "xtrain, ytrain = train.drop(\"SalePrice\", axis=1), train[\"SalePrice\"] #determino quien es X y quien y train\n",
    "\n",
    "#2)\n",
    "column_selector = ColumnTransformer(  #esto es para agrupar varias transformaciones en una instruccion , el orden es \n",
    "    #nombre de la trnaformacion que yo le ponga, transformacion a aplicar, y el nombre de las columnas donde se aplica\n",
    "    #la transofmracion o en su defecto el tipo de columnas via un make_column_selector\n",
    "    [(\"label_bin\", OneHotEncoder(sparse=False), [\"MSZoning\", \"SaleCondition\",\"LotConfig\",\"BldgType\"]), #para creae una matriz de 0 y 1\n",
    "     (\"numeric\", SimpleImputer(), make_column_selector(dtype_include=[\"float\",\"integer\"])), #para quitar NAN con promedios\n",
    "     (\"scaler\", StandardScaler(),make_column_selector(dtype_include=\"integer\")), #mis x's tiene media 0 y varianza 1\n",
    "     (\"normalizer\",Normalizer(),make_column_selector(dtype_include=\"integer\")), #mis x's van de 0 a 1\n",
    "     #(\"poly_features\", PolynomialFeatures(degree=3, include_bias=False),make_column_selector(dtype_include=\"integer\")) #el false me dice que no meta interceptos\n",
    "])\n",
    "\n",
    "\n",
    "#3)#creo el pipeline con el modelo a utilizar\n",
    "pipe = Pipeline([ \n",
    "    (\"select_cols\", column_selector), \n",
    "    (\"linmodel\",LinearRegression())\n",
    "])\n",
    "\n",
    "#4)\n",
    "#aplico cross validate\n",
    "cv = cross_validate(pipe, xtrain, ytrain, cv=5, scoring=[\"r2\", \"neg_mean_absolute_error\"]) \n",
    "r2_lin = cv[\"test_r2\"]\n",
    "mae_lin = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "#4')\n",
    "#aplico el gsearch\n",
    "#gsearch = GridSearchCV(estimator=pipe,\n",
    "                       #param_grid={'lasso__alpha': [0.1 * 10 ** (i/2) for i in range(1, 11)]},\n",
    "                       #scoring=\"neg_mean_absolute_error\", \n",
    "                       #return_train_score=True, \n",
    "                       #cv=10)\n",
    "#gsearch.fit(xtrain, ytrain) #le meto mi info al modelo de gsearch\n",
    "\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.cv_results_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_estimator_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_score_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_params_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_index_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['param_lasso__alpha'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_train_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_test_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "#5)aplico cv sobre el mejor modelo de gsearch\n",
    "#cv = cross_validate(gsearch.best_estimator_, xtrain, ytrain, cv=5, scoring=[\"r2\", \"neg_mean_absolute_error\"])\n",
    "#r2 = cv[\"test_r2\"]\n",
    "#mae = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "print(f\"mean r2: {r2_lin.mean():0.2f}\")\n",
    "print(f\"mean mae: {mae_lin.mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 491414873912.293, tolerance: 578102265.4735652\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 455811106444.42145, tolerance: 596964929.4500496\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 490187023200.40405, tolerance: 623784111.0517945\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 362848596038.6003, tolerance: 548707561.973069\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 419902565469.6558, tolerance: 598524846.1103536\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 491414873912.293, tolerance: 5781022654.735653\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 455811106444.42145, tolerance: 5969649294.500496\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 490187023200.40405, tolerance: 6237841110.517945\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 362848596038.6003, tolerance: 5487075619.73069\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 419902565469.6558, tolerance: 5985248461.103536\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 498041048414.53284, tolerance: 578102265.4735652\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 466609725663.47406, tolerance: 596964929.4500496\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 498557582235.02686, tolerance: 623784111.0517945\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 365620949702.7762, tolerance: 548707561.973069\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 428048584324.4711, tolerance: 598524846.1103536\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 498041048414.53284, tolerance: 5781022654.735653\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 466609725663.47406, tolerance: 5969649294.500496\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 498557582235.02686, tolerance: 6237841110.517945\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 365620949702.7762, tolerance: 5487075619.73069\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 428048584324.4711, tolerance: 5985248461.103536\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 512942799450.4573, tolerance: 578102265.4735652\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 490393540157.45435, tolerance: 596964929.4500496\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 516439706095.54767, tolerance: 623784111.0517945\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 370598216029.5385, tolerance: 548707561.973069\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 447012460507.29175, tolerance: 598524846.1103536\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 512942799450.4573, tolerance: 5781022654.735653\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 490393540157.45435, tolerance: 5969649294.500496\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 516439706095.54767, tolerance: 6237841110.517945\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 370598216029.5385, tolerance: 5487075619.73069\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 447012460507.29175, tolerance: 5985248461.103536\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 531655471514.6598, tolerance: 578102265.4735652\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 519254677509.7604, tolerance: 596964929.4500496\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 535862149569.3908, tolerance: 623784111.0517945\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 380395824721.9774, tolerance: 548707561.973069\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 474613859440.48425, tolerance: 598524846.1103536\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 531655471514.6598, tolerance: 5781022654.735653\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 519254677509.7604, tolerance: 5969649294.500496\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 535862149569.3908, tolerance: 6237841110.517945\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 380395824721.9774, tolerance: 5487075619.73069\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 474613859440.48425, tolerance: 5985248461.103536\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 433158796158.5848, tolerance: 578102265.4735652\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525220489245.7991, tolerance: 596964929.4500496\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 541432355699.1577, tolerance: 623784111.0517945\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 378355537297.33856, tolerance: 548707561.973069\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 479177456559.69073, tolerance: 598524846.1103536\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 433158796158.5848, tolerance: 5781022654.735653\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525220489245.7991, tolerance: 5969649294.500496\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 541432355699.1577, tolerance: 6237841110.517945\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 378355537297.33856, tolerance: 5487075619.73069\n",
      "  positive)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 479177456559.69073, tolerance: 5985248461.103536\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean r2: 0.79\n",
      "mean mae: 21,670.44\n"
     ]
    }
   ],
   "source": [
    "#Aplicando un modelo Lasso\n",
    "#1)\n",
    "#Determino mi X y y, asi como los train y test sets\n",
    "train, test = train_test_split(housing, test_size=0.2, random_state=2718)\n",
    "xtrain, ytrain = train.drop(\"SalePrice\", axis=1), train[\"SalePrice\"] \n",
    "\n",
    "#2)\n",
    "column_selector = ColumnTransformer(\n",
    "    [(\"label_bin\", OneHotEncoder(sparse=False), [\"MSZoning\", \"SaleCondition\",\"LotConfig\",\"BldgType\"]),\n",
    "     (\"numeric\", SimpleImputer(), make_column_selector(dtype_include=[\"float\",\"integer\"])), \n",
    "     (\"scaler\", StandardScaler(),make_column_selector(dtype_include=\"integer\")), \n",
    "     (\"normalizer\",Normalizer(),make_column_selector(dtype_include=\"integer\")), \n",
    "     #(\"poly_features\", PolynomialFeatures(degree=3, include_bias=False),make_column_selector(dtype_include=\"integer\")) #el false me dice que no meta interceptos\n",
    "])\n",
    "\n",
    "\n",
    "#3) creo el pipeline con el modelo a utilizar\n",
    "pipe = Pipeline([ \n",
    "    (\"select_cols\", column_selector), \n",
    "    (\"lasso\", Lasso(alpha=.1, tol=0.5, random_state=314))\n",
    "])\n",
    "\n",
    "#4)\n",
    "#aplico cross validate\n",
    "#cv = cross_validate(pipe, xtrain, ytrain, cv=10, scoring=[\"r2\", \"neg_mean_absolute_error\"]) \n",
    "#r2 = cv[\"test_r2\"]\n",
    "#mae = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "#4')\n",
    "#aplico el gsearch\n",
    "gsearch = GridSearchCV(estimator=pipe,\n",
    "                       param_grid={'lasso__alpha': [0.1 * 10 ** (i/2) for i in range(1, 11)],\n",
    "                                   'lasso__tol':[0.0001, 0.001, 0.1, 1]},\n",
    "                       scoring=\"neg_mean_absolute_error\", \n",
    "                       return_train_score=True, \n",
    "                       cv=5)\n",
    "gsearch.fit(xtrain, ytrain)\n",
    "\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.cv_results_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_estimator_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_score_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_params_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_index_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['param_lasso__alpha'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_train_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_test_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "#5)aplico cv sobre el mejor modelo de gsearch\n",
    "cv = cross_validate(gsearch.best_estimator_, xtrain, ytrain, cv=5, scoring=[\"r2\", \"neg_mean_absolute_error\"])\n",
    "r2_lasso = cv[\"test_r2\"]\n",
    "mae_lasso = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "print(f\"mean r2: {r2_lasso.mean():0.2f}\")\n",
    "print(f\"mean mae: {mae_lasso.mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean r2: 0.78\n",
      "mean mae: 21,967.15\n"
     ]
    }
   ],
   "source": [
    "#Aplicando un modelo Ridge\n",
    "#1)\n",
    "#Determino mi X y y, asi como los train y test sets\n",
    "train, test = train_test_split(housing, test_size=0.2, random_state=2718)\n",
    "xtrain, ytrain = train.drop(\"SalePrice\", axis=1), train[\"SalePrice\"] \n",
    "\n",
    "#2)\n",
    "column_selector = ColumnTransformer(\n",
    "    [(\"label_bin\", OneHotEncoder(sparse=False), [\"MSZoning\", \"SaleCondition\",\"LotConfig\",\"BldgType\"]),\n",
    "     (\"numeric\", SimpleImputer(), make_column_selector(dtype_include=[\"float\",\"integer\"])), \n",
    "     (\"scaler\", StandardScaler(),make_column_selector(dtype_include=\"integer\")), \n",
    "     (\"normalizer\",Normalizer(),make_column_selector(dtype_include=\"integer\")), \n",
    "     #(\"poly_features\", PolynomialFeatures(degree=3, include_bias=False),make_column_selector(dtype_include=\"integer\")) #el false me dice que no meta interceptos\n",
    "])\n",
    "\n",
    "\n",
    "#3)creo el pipeline con el modelo a utilizar\n",
    "pipe = Pipeline([ \n",
    "    (\"select_cols\", column_selector), \n",
    "    (\"ridge\", Ridge(alpha=1, solver = \"cholesky\", random_state=314)) \n",
    "])\n",
    "\n",
    "#4)\n",
    "#aplico cross validate\n",
    "#cv = cross_validate(pipe, xtrain, ytrain, cv=10, scoring=[\"r2\", \"neg_mean_absolute_error\"]) \n",
    "#r2 = cv[\"test_r2\"]\n",
    "#mae = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "#4')\n",
    "#aplico el gsearch\n",
    "gsearch = GridSearchCV(estimator=pipe,\n",
    "                       param_grid={'ridge__alpha': [0.1 * 10 ** (i/2) for i in range(1, 11)],\n",
    "                                  'ridge__tol':[0.0001, 0.001, 0.1, 1]},\n",
    "                       scoring=\"neg_mean_absolute_error\", \n",
    "                       return_train_score=True, \n",
    "                       cv=5)\n",
    "gsearch.fit(xtrain, ytrain)\n",
    "\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.cv_results_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_estimator_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_score_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_params_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_index_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['param_lasso__alpha'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_train_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_test_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "#5)aplico cv sobre el mejor modelo de gsearch\n",
    "cv = cross_validate(gsearch.best_estimator_, xtrain, ytrain, cv=5, scoring=[\"r2\", \"neg_mean_absolute_error\"])\n",
    "r2_ridge = cv[\"test_r2\"]\n",
    "mae_ridge = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "print(f\"mean r2: {r2_ridge.mean():0.2f}\")\n",
    "print(f\"mean mae: {mae_ridge.mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean r2: 0.72\n",
      "mean mae: 26,653.21\n"
     ]
    }
   ],
   "source": [
    "#Aplicando un modelo de Decision Tree\n",
    "#1)\n",
    "#Determino mi X y y, asi como los train y test sets\n",
    "train, test = train_test_split(housing, test_size=0.2, random_state=2718)\n",
    "xtrain, ytrain = train.drop(\"SalePrice\", axis=1), train[\"SalePrice\"] \n",
    "\n",
    "#2)\n",
    "column_selector = ColumnTransformer(\n",
    "    [(\"label_bin\", OneHotEncoder(sparse=False), [\"MSZoning\", \"SaleCondition\",\"LotConfig\",\"BldgType\"]),\n",
    "     (\"numeric\", SimpleImputer(), make_column_selector(dtype_include=[\"float\",\"integer\"])), \n",
    "     (\"scaler\", StandardScaler(),make_column_selector(dtype_include=\"integer\")), \n",
    "     (\"normalizer\",Normalizer(),make_column_selector(dtype_include=\"integer\")), \n",
    "     #(\"poly_features\", PolynomialFeatures(degree=3, include_bias=False),make_column_selector(dtype_include=\"integer\")) #el false me dice que no meta interceptos\n",
    "])\n",
    "\n",
    "\n",
    "#3)creo el pipeline con el modelo a utilizar\n",
    "pipe = Pipeline([ \n",
    "    (\"select_cols\", column_selector), \n",
    "    (\"tree\", DecisionTreeRegressor(criterion= \"mae\",max_depth = 2, random_state=314)) \n",
    "])\n",
    "\n",
    "#4)\n",
    "#aplico cross validate\n",
    "#cv = cross_validate(pipe, xtrain, ytrain, cv=10, scoring=[\"r2\", \"neg_mean_absolute_error\"]) \n",
    "#r2 = cv[\"test_r2\"]\n",
    "#mae = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "#4')\n",
    "#aplico el gsearch\n",
    "gsearch = GridSearchCV(estimator=pipe,\n",
    "                       param_grid={'tree__max_depth':[2,3,4,5]},\n",
    "                       scoring=\"neg_mean_absolute_error\", \n",
    "                       return_train_score=True, \n",
    "                       cv=5)\n",
    "gsearch.fit(xtrain, ytrain)\n",
    "\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.cv_results_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_estimator_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_score_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_params_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_index_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['param_lasso__alpha'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_train_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_test_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "#5)aplico cv sobre el mejor modelo de gsearch\n",
    "cv = cross_validate(gsearch.best_estimator_, xtrain, ytrain, cv=5, scoring=[\"r2\", \"neg_mean_absolute_error\"])\n",
    "r2_dt = cv[\"test_r2\"]\n",
    "mae_dt = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "print(f\"mean r2: {r2_dt.mean():0.2f}\")\n",
    "print(f\"mean mae: {mae_dt.mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean r2: 0.69\n",
      "mean mae: 27,326.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio.delacruz/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Aplicando un modelo de SVR Lineal\n",
    "#1)\n",
    "#Determino mi X y y, asi como los train y test sets\n",
    "train, test = train_test_split(housing, test_size=0.2, random_state=2718)\n",
    "xtrain, ytrain = train.drop(\"SalePrice\", axis=1), train[\"SalePrice\"] \n",
    "\n",
    "#2)\n",
    "column_selector = ColumnTransformer(\n",
    "    [(\"label_bin\", OneHotEncoder(sparse=False), [\"MSZoning\", \"SaleCondition\",\"LotConfig\",\"BldgType\"]),\n",
    "     (\"numeric\", SimpleImputer(), make_column_selector(dtype_include=[\"float\",\"integer\"])), \n",
    "     (\"scaler\", StandardScaler(),make_column_selector(dtype_include=\"integer\")), \n",
    "     (\"normalizer\",Normalizer(),make_column_selector(dtype_include=\"integer\")), \n",
    "     #(\"poly_features\", PolynomialFeatures(degree=3, include_bias=False),make_column_selector(dtype_include=\"integer\")) #el false me dice que no meta interceptos\n",
    "])\n",
    "\n",
    "\n",
    "#3)creo el pipeline con el modelo a utilizar\n",
    "pipe = Pipeline([ \n",
    "    (\"select_cols\", column_selector), \n",
    "    (\"svr\", LinearSVR(epsilon =1.5, C=1, tol =.0001, random_state=314)) \n",
    "])\n",
    "\n",
    "#4)\n",
    "#aplico cross validate\n",
    "#cv = cross_validate(pipe, xtrain, ytrain, cv=10, scoring=[\"r2\", \"neg_mean_absolute_error\"]) \n",
    "#r2 = cv[\"test_r2\"]\n",
    "#mae = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "#4')\n",
    "#aplico el gsearch\n",
    "gsearch = GridSearchCV(estimator=pipe,\n",
    "                       param_grid={'svr__epsilon':[0,1,1.5,2],\n",
    "                                  'svr__C' : [1,20,50,100],\n",
    "                                  'svr__tol': [.001, .01, .1, 1]},\n",
    "                       scoring=\"neg_mean_absolute_error\", \n",
    "                       return_train_score=True, \n",
    "                       cv=5)\n",
    "gsearch.fit(xtrain, ytrain)\n",
    "\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.cv_results_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_estimator_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_score_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_params_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_index_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['param_lasso__alpha'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_train_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_test_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "#5)aplico cv sobre el mejor modelo de gsearch\n",
    "cv = cross_validate(gsearch.best_estimator_, xtrain, ytrain, cv=5, scoring=[\"r2\", \"neg_mean_absolute_error\"])\n",
    "r2_svrl = cv[\"test_r2\"]\n",
    "mae_svrl = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "print(f\"mean r2: {r2_svrl.mean():0.2f}\")\n",
    "print(f\"mean mae: {mae_svrl.mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean r2: -0.03\n",
      "mean mae: 55,241.42\n"
     ]
    }
   ],
   "source": [
    "#Aplicando un modelo de SVR No Lineal\n",
    "#1)\n",
    "#Determino mi X y y, asi como los train y test sets\n",
    "train, test = train_test_split(housing, test_size=0.2, random_state=2718)\n",
    "xtrain, ytrain = train.drop(\"SalePrice\", axis=1), train[\"SalePrice\"] \n",
    "\n",
    "#2)\n",
    "column_selector = ColumnTransformer(\n",
    "    [(\"label_bin\", OneHotEncoder(sparse=False), [\"MSZoning\", \"SaleCondition\",\"LotConfig\",\"BldgType\"]),\n",
    "     (\"numeric\", SimpleImputer(), make_column_selector(dtype_include=[\"float\",\"integer\"])), \n",
    "     (\"scaler\", StandardScaler(),make_column_selector(dtype_include=\"integer\")), \n",
    "     (\"normalizer\",Normalizer(),make_column_selector(dtype_include=\"integer\")), \n",
    "     #(\"poly_features\", PolynomialFeatures(degree=3, include_bias=False),make_column_selector(dtype_include=\"integer\")) #el false me dice que no meta interceptos\n",
    "])\n",
    "\n",
    "\n",
    "#3)creo el pipeline con el modelo a utilizar\n",
    "pipe = Pipeline([ \n",
    "    (\"select_cols\", column_selector), \n",
    "    (\"svrp\", SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)) \n",
    "])\n",
    "\n",
    "#4)\n",
    "#aplico cross validate\n",
    "#cv = cross_validate(pipe, xtrain, ytrain, cv=10, scoring=[\"r2\", \"neg_mean_absolute_error\"]) \n",
    "#r2 = cv[\"test_r2\"]\n",
    "#mae = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "#4')\n",
    "#aplico el gsearch\n",
    "gsearch = GridSearchCV(estimator=pipe,\n",
    "                       param_grid={'svrp__epsilon':[0,1,1.5,2],\n",
    "                                   'svrp__degree' :[2,3],\n",
    "                                   'svrp__C': [1,20,50,100],\n",
    "                                   'svrp__tol': [.001, .01, .1, 1]},\n",
    "                       scoring=\"neg_mean_absolute_error\", \n",
    "                       return_train_score=True, \n",
    "                       cv=5)\n",
    "gsearch.fit(xtrain, ytrain)\n",
    "\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.cv_results_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_estimator_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_score_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_params_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_index_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['param_lasso__alpha'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_train_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_test_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "#5) aplico cv sobre el mejor modelo de gsearch\n",
    "cv = cross_validate(gsearch.best_estimator_, xtrain, ytrain, cv=5, scoring=[\"r2\", \"neg_mean_absolute_error\"])\n",
    "r2_svr = cv[\"test_r2\"]\n",
    "mae_svr = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "print(f\"mean r2: {r2_svr.mean():0.2f}\")\n",
    "print(f\"mean mae: {mae_svr.mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean r2: 0.82\n",
      "mean mae: 19,546.72\n"
     ]
    }
   ],
   "source": [
    "#Aplicando un modelo de Voting\n",
    "#1)\n",
    "#Determino mi X y y, asi como los train y test sets\n",
    "train, test = train_test_split(housing, test_size=0.2, random_state=2718)\n",
    "xtrain, ytrain = train.drop(\"SalePrice\", axis=1), train[\"SalePrice\"] \n",
    "\n",
    "#2)\n",
    "column_selector = ColumnTransformer(\n",
    "    [(\"label_bin\", OneHotEncoder(sparse=False), [\"MSZoning\", \"SaleCondition\",\"LotConfig\",\"BldgType\"]),\n",
    "     (\"numeric\", SimpleImputer(), make_column_selector(dtype_include=[\"float\",\"integer\"])), \n",
    "     (\"scaler\", StandardScaler(),make_column_selector(dtype_include=\"integer\")), \n",
    "     (\"normalizer\",Normalizer(),make_column_selector(dtype_include=\"integer\")), \n",
    "     #(\"poly_features\", PolynomialFeatures(degree=2, include_bias=False),make_column_selector(dtype_include=\"integer\")) #el false me dice que no meta interceptos\n",
    "])\n",
    "\n",
    "\n",
    "#3)creo el pipeline con el modelo a utilizar\n",
    "m1 = LinearRegression()\n",
    "m2 = RandomForestRegressor(n_estimators=20, random_state=314,criterion='mae',max_depth=5) #estimators 10\n",
    "m3 = DecisionTreeRegressor(criterion= \"mae\",max_depth = 5, random_state=314) #max depth 2\n",
    "m4 = LinearSVR(epsilon =2, C=5, tol =.0001, random_state=314) #epsilo 1.5, c 1\n",
    "pipe = Pipeline([ \n",
    "    (\"select_cols\", column_selector), \n",
    "    (\"vr\", VotingRegressor([('lr', m1),('rf', m2),('dt', m3),('sv', m4)], weights=None, n_jobs=-1)) \n",
    "])\n",
    "\n",
    "#4)\n",
    "#aplico cross validate\n",
    "cv = cross_validate(pipe, xtrain, ytrain, cv=10, scoring=[\"r2\", \"neg_mean_absolute_error\"]) \n",
    "r2_v = cv[\"test_r2\"]\n",
    "mae_v = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "#4')\n",
    "#aplico el gsearch\n",
    "#gsearch = GridSearchCV(estimator=pipe,\n",
    "#                       param_grid={'vr__weights':[0,1]},\n",
    "#                       scoring=\"neg_mean_absolute_error\", \n",
    "#                       return_train_score=True, \n",
    "#                       cv=5)\n",
    "#gsearch.fit(xtrain, ytrain)\n",
    "\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.cv_results_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_estimator_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_score_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_params_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_index_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['param_lasso__alpha'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_train_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_test_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "#5) aplico cv sobre el mejor modelo de gsearch\n",
    "#cv = cross_validate(gsearch.best_estimator_, xtrain, ytrain, cv=5, scoring=[\"r2\", \"neg_mean_absolute_error\"])\n",
    "#r2 = cv[\"test_r2\"]\n",
    "#mae = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "print(f\"mean r2: {r2_v.mean():0.2f}\")\n",
    "print(f\"mean mae: {mae_v.mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-20-787dcbbe8d5f>, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-787dcbbe8d5f>\"\u001b[0;36m, line \u001b[0;32m62\u001b[0m\n\u001b[0;31m    r2_b = cv[\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#Aplicando un modelo de Boosting\n",
    "#1)\n",
    "#Determino mi X y y, asi como los train y test sets\n",
    "train, test = train_test_split(housing, test_size=0.2, random_state=2718)\n",
    "xtrain, ytrain = train.drop(\"SalePrice\", axis=1), train[\"SalePrice\"] \n",
    "\n",
    "#2)\n",
    "column_selector = ColumnTransformer(\n",
    "    [(\"label_bin\", OneHotEncoder(sparse=False), [\"MSZoning\", \"SaleCondition\",\"LotConfig\",\"BldgType\"]),\n",
    "     (\"numeric\", SimpleImputer(), make_column_selector(dtype_include=[\"float\",\"integer\"])), \n",
    "     (\"scaler\", StandardScaler(),make_column_selector(dtype_include=\"integer\")), \n",
    "     (\"normalizer\",Normalizer(),make_column_selector(dtype_include=\"integer\")), \n",
    "     #(\"poly_features\", PolynomialFeatures(degree=3, include_bias=False),make_column_selector(dtype_include=\"integer\")) #el false me dice que no meta interceptos\n",
    "])\n",
    "\n",
    "\n",
    "#3)creo el pipeline con el modelo a utilizar\n",
    "n1 = LinearRegression()\n",
    "n2 = RandomForestRegressor(n_estimators=20, random_state=314,criterion='mae',max_depth=5) #estimators 10\n",
    "n3 = DecisionTreeRegressor(criterion= \"mae\",max_depth = 50, random_state=314) #max depth 2\n",
    "n4 = LinearSVR(epsilon =2, C=5, tol =.0001, random_state=314) #epsilo 1.5, c 1\n",
    "pipe = Pipeline([ \n",
    "    (\"select_cols\", column_selector), \n",
    "    (\"br\", BaggingRegressor(base_estimator=None, n_estimators=10, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=314)) \n",
    "])\n",
    "\n",
    "#4)\n",
    "#aplico cross validate\n",
    "#cv = cross_validate(pipe, xtrain, ytrain, cv=10, scoring=[\"r2\", \"neg_mean_absolute_error\"]) \n",
    "#r2 = cv[\"test_r2\"]\n",
    "#mae = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "#4')\n",
    "#aplico el gsearch\n",
    "gsearch = GridSearchCV(estimator=pipe,\n",
    "                       param_grid={'br__base_estimator':[n1,n2,n3,n4]},\n",
    "                       scoring=\"neg_mean_absolute_error\", \n",
    "                       return_train_score=True, \n",
    "                       cv=5)\n",
    "gsearch.fit(xtrain, ytrain)\n",
    "\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.cv_results_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_estimator_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_score_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_params_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_index_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['param_lasso__alpha'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_train_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_test_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "#5) aplico cv sobre el mejor modelo de gsearch\n",
    "cv = cross_validate(gsearch.best_estimator_, xtrain, ytrain, cv=5, scoring=[\"r2\", \"neg_mean_absolute_error\"])\n",
    "r2_b = cv["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando un modelo de RandomForest\n",
    "#1)\n",
    "#Determino mi X y y, asi como los train y test sets\n",
    "train, test = train_test_split(housing, test_size=0.2, random_state=2718)\n",
    "xtrain, ytrain = train.drop(\"SalePrice\", axis=1), train[\"SalePrice\"] \n",
    "\n",
    "#2)\n",
    "column_selector = ColumnTransformer(\n",
    "    [(\"label_bin\", OneHotEncoder(sparse=False), [\"MSZoning\", \"SaleCondition\",\"LotConfig\",\"BldgType\"]),\n",
    "     (\"numeric\", SimpleImputer(), make_column_selector(dtype_include=[\"float\",\"integer\"])), \n",
    "     (\"scaler\", StandardScaler(),make_column_selector(dtype_include=\"integer\")), \n",
    "     (\"normalizer\",Normalizer(),make_column_selector(dtype_include=\"integer\")), \n",
    "     #(\"poly_features\", PolynomialFeatures(degree=3, include_bias=False),make_column_selector(dtype_include=\"integer\")) #el false me dice que no meta interceptos\n",
    "])\n",
    "\n",
    "\n",
    "#3)creo el pipeline con el modelo a utilizar\n",
    "\n",
    "pipe = Pipeline([ \n",
    "    (\"select_cols\", column_selector), \n",
    "    (\"rf\",  RandomForestRegressor(max_depth=None, criterion='mae',n_jobs=-1,random_state=314)) \n",
    "])\n",
    "\n",
    "#4)\n",
    "#aplico cross validate\n",
    "#cv = cross_validate(pipe, xtrain, ytrain, cv=10, scoring=[\"r2\", \"neg_mean_absolute_error\"]) \n",
    "#r2 = cv[\"test_r2\"]\n",
    "#mae = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "#4')\n",
    "#aplico el gsearch\n",
    "gsearch = GridSearchCV(estimator=pipe,\n",
    "                       param_grid={'rf__max_depth':[2,4,5,10,20,50,100]},\n",
    "                       scoring=\"neg_mean_absolute_error\", \n",
    "                       return_train_score=True, \n",
    "                       cv=5)\n",
    "gsearch.fit(xtrain, ytrain)\n",
    "\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.cv_results_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_estimator_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_score_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_params_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(gsearch.best_index_) \n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['param_lasso__alpha'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_train_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "#print(cvres['mean_test_score'])\n",
    "#print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "#5) aplico cv sobre el mejor modelo de gsearch\n",
    "cv = cross_validate(gsearch.best_estimator_, xtrain, ytrain, cv=5, scoring=[\"r2\", \"neg_mean_absolute_error\"])\n",
    "r2_rf2 = cv[\"test_r2\"]\n",
    "mae_rf2 = -cv[\"test_neg_mean_absolute_error\"]\n",
    "\n",
    "print(f\"mean r2: {r2_rf2.mean():0.2f}\")\n",
    "print(f\"mean mae: {mae_rf2.mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se puede apreciar que el modelo que maximizo la r2 y minimizo el mae es el random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
